{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 2,
    "file_extension": ".py",
    "mimetype": "text/x-python",
    "name": "python",
    "npconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": 3,
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "colab_version_clean_pipeline.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/joshuaghannan/ECEC247_Project/blob/jonathan-tests/colab_version_clean_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JA1x8mSiTyYL",
        "colab_type": "text"
      },
      "source": [
        "### loading modules and data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQwpQEszAQ9u",
        "colab_type": "code",
        "outputId": "ac103e35-763f-42e4-d1d8-407f6bd0ff3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "########################################################\n",
        "\n",
        "# If running with Google Colab\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g6iA6D4yYwiq",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "outputId": "22c5fd05-680a-4f3d-d791-dd9f0f5f465c"
      },
      "source": [
        "# # If using colab\n",
        " from google.colab import files\n",
        " files.upload()\n",
        "# # select the 3 .py files (models, utils, data_utils)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ac52e602-60f7-42a1-8501-e6698f694c08\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-ac52e602-60f7-42a1-8501-e6698f694c08\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving data_utils.py to data_utils.py\n",
            "Saving models.py to models.py\n",
            "Saving utils.py to utils.py\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'data_utils.py': b'import numpy as np\\nimport scipy.signal as sig\\nimport pywt\\nfrom torch.utils.data import Dataset, DataLoader\\nimport torch\\n\\n\\'\\'\\'\\nThe customized dataset and data augmentations are defined here.\\n\\n\\'\\'\\'\\nclass EEG_Dataset(Dataset):\\n    \\'\\'\\'\\n    use use fold_idx to instantiate different train val datasets for k-fold cross validation\\n\\n    \\'\\'\\'\\n    def __init__ (self, X_train=None, y_train=None, p_train=None, X_val=None, y_val=None, p_val=None, X_test=None, y_test=None, p_test=None, mode=\\'train\\'):\\n        if mode == \\'train\\':\\n            self.X = X_train\\n            self.y = y_train- 769\\n            self.p = p_train\\n            \\n        elif mode == \\'val\\':\\n            self.X = X_val\\n            self.y = y_val- 769\\n            self.p = p_val\\n\\n        elif mode == \\'test\\':\\n            self.X = X_test\\n            self.y = y_test - 769        \\n            self.p = p_test\\n\\n    def __len__(self):\\n        return (self.X.shape[0])\\n    \\n    def __getitem__(self, idx):\\n        \\'\\'\\'\\n        X: (augmented) time sequence \\n        y: class label\\n        p: person id\\n\\n        \\'\\'\\'\\n        X = torch.from_numpy(self.X[idx,:,:]).float()\\n        y = torch.tensor(self.y[idx]).long()\\n        p = torch.tensor(self.p[idx]).long()\\n        #p = torch.from_numpy(self.p[idx,:]).long()     \\n        sample = {\\'X\\': X, \\'y\\': y, \\'p\\':p}\\n\\n        return sample\\n\\n\\n\\'\\'\\'\\nData Augmentions\\n\\n\\'\\'\\'\\ndef standardize(X, mu, std):\\n    return (X - mu / std)\\n\\n\\ndef window_data(X, y, p, window_size, stride):\\n  \\'\\'\\'\\n  X (a 3-d tensor) of size (#trials, #electrodes, #time series)\\n  y (#trials,): label \\n  p (#trials, 1): person id\\n\\n  X_new1: The first output stacks the windowed data in a new dimension, resulting \\n    in a 4-d tensor of size (#trials x #electrodes x #windows x #window_size).\\n  X_new2: The second option makes the windows into new trails, resulting in a new\\n    X tensor of size (#trials*#windows x #electrodes x #window_size). To account \\n    for the larger number of trials, we also need to augment the y data.\\n  y_new: The augmented y vector of size (#trials*#windows) to match X_new2.\\n  p_new: The augmented p vector of size (#trials*#windows) to match X_new2\\n \\n  \\'\\'\\'\\n  num_sub_trials = int((X.shape[2]-window_size)/stride)\\n  X_new1 = np.empty([X.shape[0],X.shape[1],num_sub_trials,window_size])\\n  X_new2 = np.empty([X.shape[0]*num_sub_trials,X.shape[1],window_size])\\n  y_new = np.empty([X.shape[0]*num_sub_trials])\\n  p_new = np.empty([X.shape[0]*num_sub_trials])\\n  for i in range(X.shape[0]):\\n    for j in range(X.shape[1]):\\n      for k in range(num_sub_trials):\\n        X_new1[i,j,k:k+window_size]    = X[i,j,k*stride:k*stride+window_size]\\n        X_new2[i*num_sub_trials+k,j,:] = X[i,j,k*stride:k*stride+window_size]\\n        y_new[i*num_sub_trials+k] = y[i]\\n        p_new[i*num_sub_trials+k] = p[i]\\n  return X_new1, X_new2, y_new, p_new\\n\\n\\ndef stft_data(X, window, stride):\\n    \\'\\'\\'\\n    Inputs:\\n    X - input data, last dimension is one which transform will be taken across.\\n    window - size of sliding window to take transform across\\n    stride - stride of sliding window across time-series\\n\\n    Returns:\\n    X_STFT - Output data, same shape as input with last dimension replaced with two new dimensions, F x T.\\n            where F = window//2 + 1 is the frequency axis\\n            and T = (input_length - window)//stride + 1, similar to the formula for aconvolutional filter.\\n    t - the corresponding times for the time axis, T\\n    f - the corresponding frequencies on the frequency axis, F.\\n\\n    reshape X_STFT (N, C, F, T) to (N, C*F, T) to fit the input of rnn\\n\\n    Note that a smaller window means only higher frequencies may be found, but give finer time resolution.\\n    Conversely, a large window gives better frequency resolution, but poor time resolution.\\n\\n    \\'\\'\\'\\n    noverlap = window-stride\\n    if noverlap < 0 :\\n        print(\\'Stride results in skipped data!\\')\\n        return\\n    f, t, X_STFT = sig.spectrogram(X,nperseg=window,noverlap=noverlap,fs=250, return_onesided=True)\\n    N, C, F, T = X_STFT.shape\\n    X_STFT = X_STFT.reshape(N, C*F, T)\\n    return X_STFT\\n\\n\\ndef cwt_data(X, num_levels, top_scale=3):\\n    \\'\\'\\'\\n    Takes in data, computes CWT using the mexican hat or ricker wavelet using scipy\\n    Also takes in the top scale parameter.  I use logspace, so scale goes from 1 -> 2^top_scale with num_levels steps.\\n    Appends to the data a new dimension, of size \\'num_levels\\'\\n    New dimension corresponds to wavelet content at num_levels different scalings (linear)\\n    also returns the central frequencies that the scalings correspond to\\n    input data is N x C X T\\n    output data is N x C x T x F\\n    note: CWT is fairly slow to compute\\n\\n    # EXAMPLE USAGE\\n    test, freqs = cwt_data(X_train_valid[0:5,:,:],num_levels=75,top_scale=4)\\n    \\'\\'\\'\\n    scales = np.logspace(start=0,stop=top_scale,num=num_levels)\\n    out = np.empty((X.shape[0],X.shape[1],X.shape[2],num_levels))\\n    for i in range(X.shape[0]):\\n        for j in range(X.shape[1]):\\n            coef = sig.cwt(X[i,j,:],sig.ricker,scales)\\n            out[i,j,:] = coef.T\\n    freqs = pywt.scale2frequency(\\'mexh\\',scales)*250\\n    N, C, T, F = out.shape\\n    X_CWT = np.transpose(out, (0,1,3,2)).reshape(N, C*F, T)\\n    return X_CWT\\n\\n\\ndef Aug_Data(X, y, p, aug_type=None, window_size=200, window_stride=20, stft_size=None, stft_stride=None, cwt_level=None, cwt_scale=None):\\n    if aug_type == None:\\n        X_aug, y_aug, p_aug = X, y, p\\n    elif aug_type == \"window\":\\n        _, X_aug, y_aug, p_aug = window_data(X, y, p, window_size, window_stride)\\n    elif aug_type == \"stft\":\\n        X_aug = stft_data(X, stft_size, stft_stride)\\n        y_aug, p_aug = y, p\\n    elif aug_type == \\'cwt\\':\\n        X_aug = cwt_data(X, cwt_level, cwt_scale)\\n        y_aug, p_aug = y, p\\n    return X_aug, y_aug, p_aug\\n\\n\\'\\'\\'\\nSplit training and validation\\n\\n\\'\\'\\'\\n\\ndef Train_Val_Data(X_train_valid, y_train_val):\\n    \\'\\'\\'\\n    split the train_valid into k folds (we fix k = 5 here)\\n    return: list of index of train data and val data of k folds\\n    train_fold[i], val_fold[i] is the index for training and validation in the i-th fold \\n\\n    \\'\\'\\'\\n    fold_idx = []\\n    train_fold = []\\n    val_fold = []\\n    train_val_num = X_train_valid.shape[0]\\n    fold_num = int(train_val_num / 5)\\n    perm = np.random.permutation(train_val_num)\\n    for k in range(5):\\n        fold_idx.append(np.arange(k*fold_num, (k+1)*fold_num, 1))\\n    for k in range(5):\\n        val_fold.append(fold_idx[k])\\n        count = 0\\n        for i in range(5):\\n            if i != k:\\n                if count == 0:\\n                    train_idx = fold_idx[i]\\n                else:\\n                    train_idx = np.concatenate((train_idx, fold_idx[i]))\\n                count += 1\\n        train_fold.append(train_idx)\\n\\n    return train_fold, val_fold',\n",
              " 'models.py': b\"import numpy as np\\nimport torch\\nimport torch.nn as nn\\n\\n'''\\nAdd your new models here as a new class\\n\\n'''\\n\\nclass LSTMnet(nn.Module):\\n    '''\\n    Create Basic LSTM:\\n    2 layers\\n\\n    '''\\n    def __init__(self, input_size, hidden_size, output_dim, dropout):\\n        super(LSTMnet, self).__init__()\\n        self.input_size = input_size\\n        self.hidden_size = hidden_size\\n        self.output_dim = output_dim\\n        self.rnn = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=2, dropout=dropout)\\n        self.fc = nn.Linear(hidden_size, output_dim)\\n    \\n    def forward(self, x, h=None):\\n        x = x.permute(2, 0, 1)\\n        if type(h) == type(None):\\n            out, hn = self.rnn(x)\\n        else:\\n            out, hn = self.rnn(x, h.detach())\\n        out = self.fc(out[-1, :, :])\\n        return out\\n\\n\\nclass CNNLSTMnet(nn.Module):\\n    '''\\n    CNN + LSTM\\n    \\n    '''\\n    def __init__(self, cnn_input_size, rnn_input_size, hidden_size, output_dim, dropout):\\n        super(CNNLSTMnet, self).__init__()\\n        self.cnn_input_size = cnn_input_size\\n        self.rnn_input_size = rnn_input_size\\n        self.hidden_size = hidden_size\\n        self.output_dim = output_dim\\n        self.cnn = nn.Sequential(\\n            nn.Conv1d(cnn_input_size, rnn_input_size, kernel_size=10, stride=2),\\n            nn.BatchNorm1d(rnn_input_size),\\n            nn.ReLU(),\\n        )\\n        self.rnn = nn.LSTM(input_size=rnn_input_size, hidden_size=hidden_size, num_layers=2, dropout=dropout)\\n        self.fc = nn.Linear(hidden_size, output_dim)\\n    \\n    def forward(self, x, h=None):\\n        out = self.cnn(x)\\n        out = out.permute(2,0,1)\\n        if type(h) == type(None):\\n            out, hn = self.rnn(out)\\n        else:\\n            out, hn = self.rnn(out, h.detach())\\n        out = self.fc(out[-1, :, :])\\n        return out\\n\\nclass CNN2LSTM(nn.Module):\\n    '''\\n    CNN1 + LSTM1 + CNN2 + LSTM2\\n    \\n    '''\\n    def __init__(self, cnn1_input_size, rnn1_input_size, hidden_size1, cnn2_input_size, rnn2_input_size, hidden_size2, output_dim, dropout):\\n        super(CNNLSTMnet, self).__init__()\\n        self.cnn_input_size = cnn_input_size\\n        self.rnn_input_size = rnn_input_size\\n        self.hidden_size = hidden_size\\n        self.output_dim = output_dim\\n        self.cnn1 = nn.Sequential(\\n            nn.Conv1d(cnn1_input_size, rnn1_input_size, kernel_size=10, stride=2),\\n            nn.BatchNorm1d(rnn_input_size),\\n            nn.ReLU(),\\n        )\\n        self.rnn1 = nn.LSTM(input_size=rnn1_input_size, hidden_size=hidden_size1, num_layers=1, dropout=dropout)\\n        self.cnn2 = nn.Sequential(\\n            nn.Conv1d(cnn2_input_size, rnn2_input_size, kernel_size=10, stride=2),\\n            nn.BatchNorm1d(rnn2_input_size),\\n            nn.ReLU(),\\n        )\\n        self.rnn2 = nn.LSTM(input_size=rnn2_input_size, hidden_size=hidden_size2, num_layers=1, dropout=dropout)\\n        self.fc = nn.Linear(hidden_size2, output_dim)\\n    \\n    def forward(self, x, h=None):\\n        out = self.cnn1(x)\\n        # out (batch, feature_num, seq_len)\\n        out = out.permute(2,0,1)\\n        # out (seq_len, batch, feature_num)\\n        if type(h) == type(None):\\n            out, hn = self.rnn(out)\\n        else:\\n            out, hn = self.rnn(out, h.detach())        \\n        # out (seq_len, batch, feature_num)\\n        out = out.permute(1, 2, 0)\\n        # out (batch, feature_num, seq_len)\\n\\n\\n        out = self.fc(out[-1, :, :])\\n        return out\\n\\nclass GRUnet(nn.Module):\\n    '''\\n    Create Basic GRU:\\n    2 layers\\n    \\n    '''\\n    def __init__(self, input_size, hidden_size, output_dim, dropout):\\n        super(GRUnet, self).__init__()\\n        self.input_size = input_size\\n        self.hidden_size = hidden_size\\n        self.output_dim = output_dim\\n        self.rnn = nn.GRU(input_size=input_size, hidden_size=hidden_size, num_layers=2, dropout=dropout)\\n        self.fc = nn.Linear(hidden_size, output_dim)\\n    \\n    def forward(self, x, h=None):\\n        x = x.permute(2, 0, 1)\\n        if type(h) == type(None):\\n            out, hn = self.rnn(x)\\n        else:\\n            out, hn = self.rnn(x, h.detach())\\n        out = self.fc(out[-1, :, :])\\n        return out\\n\\nclass CNNGRUnet(nn.Module):\\n    '''\\n    CNN + GRU\\n    \\n    '''\\n    def __init__(self, cnn_input_size, rnn_input_size, hidden_size, output_dim, dropout):\\n        super(CNNGRUnet, self).__init__()\\n        self.cnn_input_size = cnn_input_size\\n        self.rnn_input_size = rnn_input_size\\n        self.hidden_size = hidden_size\\n        self.output_dim = output_dim\\n        self.cnn = nn.Sequential(\\n            nn.Conv1d(cnn_input_size, rnn_input_size, kernel_size=40, stride=2),\\n            nn.BatchNorm1d(rnn_input_size),\\n            nn.ReLU(),\\n        )\\n        self.rnn = nn.GRU(input_size=rnn_input_size, hidden_size=hidden_size, num_layers=2, dropout=dropout)\\n        self.fc = nn.Linear(hidden_size, output_dim)\\n    \\n    def forward(self, x, h=None):\\n        out = self.cnn(x)\\n        out = out.permute(2,0,1)\\n        if type(h) == type(None):\\n            out, hn = self.rnn(out)\\n        else:\\n            out, hn = self.rnn(out, h.detach())\\n        out = self.fc(out[-1, :, :])\\n        return out\",\n",
              " 'utils.py': b'import numpy as np\\nimport torch\\nimport time\\nfrom models import *\\nfrom data_utils import *\\n\\nis_cuda = torch.cuda.is_available()\\nif is_cuda:\\n    device = torch.device(\"cuda:0\")\\n    print(\"GPU is available\")\\nelse:\\n    device = torch.device(\"cpu\")\\n    print(\"GPU not available, CPU used\")\\n\\ndef InitRNN(rnn_type=\"LSTM\", input_size=22, rnn_input_size=40, hidden_size=50, output_dim=4, dropout=0.5, lr=1e-3, weight_decay=1e-3):\\n    \\'\\'\\'\\n    Function to initialize RNN\\n    \\n    input: RNN type(LSTM, GRU, CNNLSTM), and other params if neccessary (regularization, acitvation, dropout, num layers, etc.)\\n\\n    output: model, criterion, optimizer\\n\\n    TODO: Eventually should also take in params such as dropout, number of layers, and activation function(s), etc.\\n    \\'\\'\\'\\n\\n    if rnn_type==\"LSTM\":\\n        model = LSTMnet(input_size=input_size, hidden_size=hidden_size, output_dim=output_dim, dropout=dropout).to(device)\\n\\n    elif rnn_type==\"GRU\":\\n        model = GRUnet(input_size=input_size, hidden_size=hidden_size, output_dim=output_dim, dropout=dropout).to(device)\\n    \\n    elif rnn_type==\"CNNLSTM\":\\n        model = CNNLSTMnet(cnn_input_size=input_size, rnn_input_size=rnn_input_size, hidden_size=hidden_size, output_dim=output_dim, dropout=dropout).to(device)\\n\\n    elif rnn_type==\"CNNGRU\":\\n        model = CNNGRUnet(cnn_input_size=input_size, rnn_input_size=rnn_input_size, hidden_size=hidden_size, output_dim=output_dim, dropout=dropout).to(device)\\n\\n    criterion = nn.CrossEntropyLoss()\\n    optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\\n\\n    return model, criterion, optimizer\\n\\n\\ndef TrainValRNN(model, criterion, optimizer, trainloader, valloader=None, num_epochs=20, verbose=True, aug_type=None, window_size=None, vote_num=None):\\n    val_acc_list = []\\n    best_val_acc = 0.0\\n    for ep in range(num_epochs):\\n        tstart = time.time()\\n        running_loss = 0.0\\n        correct, total = 0, 0\\n        for idx, batch in enumerate(trainloader):\\n            optimizer.zero_grad()\\n            X = batch[\\'X\\'].to(device)\\n            y = batch[\\'y\\'].to(device)\\n            output = model(X)\\n            loss = criterion(output, y)\\n            running_loss += loss.item()\\n            loss.backward()\\n            optimizer.step()\\n            pred = torch.argmax(output, dim=1)\\n            correct += torch.sum(pred == y).item()\\n            total += y.shape[0]\\n        train_acc = correct / total\\n        train_loss = running_loss\\n        \\'\\'\\'\\n        The validation need to be customized according to the data augmenation type\\n        for stft and cwt: they didn\\'t increase the number of trials, we can directly pass the augmented data to the model\\n        for window: it increase the number of trials, we need to do a voting for different subsequences in one trial\\n        \\n        \\'\\'\\'\\n        if aug_type == \\'window\\':\\n            correct, total = 0, 0\\n            for idx, batch in enumerate(valloader):\\n                #X = batch[\\'X\\'].permute(2, 0, 1).to(device)\\n                X = batch[\\'X\\'].to(device)\\n                y = batch[\\'y\\'].to(device)\\n                vote_idx = np.random.choice(1000-window_size, vote_num)\\n                vote_pred = np.zeros(y.shape[0])\\n                for i in range(len(vote_idx)):\\n                    X_sub = X[:,:,vote_idx[i]:vote_idx[i]+window_size]\\n                    output = model(X_sub)\\n                    pred = torch.argmax(output, dim=1)\\n                    if i == 0:\\n                        vote_matrix = np.asarray(pred.cpu().view(-1, 1))\\n                    else:\\n                        vote_matrix = np.hstack((vote_matrix, np.asarray(pred.cpu().view(-1,1))))\\n                for row in range(y.shape[0]):\\n                    vote_pred[row] = np.bincount(vote_matrix[row, :]).argmax()\\n                vote_pred = torch.from_numpy(vote_pred).long()\\n                correct += torch.sum(vote_pred == y.cuda()).item()\\n                total += y.shape[0]\\n            val_acc = correct / total        \\n        else:\\n            correct, total = 0, 0\\n            for idx, batch in enumerate(valloader):\\n                X = batch[\\'X\\'].to(device)\\n                y = batch[\\'y\\'].to(device)\\n                output = model(X)                    \\n                pred = torch.argmax(output, dim=1)\\n                correct += torch.sum(pred == y.cuda()).item()\\n                total += y.shape[0]\\n            val_acc = correct / total\\n        tend = time.time()\\n        if verbose:\\n            print(\\'epoch: {:<3d}    time: {:<3.2f}    loss: {:<3.3f}    train acc: {:<1.3f}    val acc: {:<1.3f}\\'.format(ep+1, tend - tstart, train_loss, train_acc, val_acc))\\n        if val_acc >= best_val_acc:\\n            best_val_acc = val_acc\\n            best_model = model\\n            print (\\'saving best model...\\')\\n    return best_model\\n\\ndef TestRNN(model, X_test, y_test, p_test, aug_type=None, window_size=None, vote_num=None):\\n    if aug_type == \\'window\\':\\n        EEG_testset = EEG_Dataset(X_test=X_test, y_test=y_test, p_test=p_test, mode=\\'test\\')\\n        EEG_testloader = DataLoader(EEG_testset, batch_size=128, shuffle=False)\\n        correct, total = 0, 0\\n        for idx, batch in enumerate(EEG_testloader):\\n            X = batch[\\'X\\'].to(device)\\n            y = batch[\\'y\\'].to(device)\\n            vote_idx = np.random.choice(1000-window_size, vote_num)\\n            vote_pred = np.zeros(y.shape[0])\\n            for i in range(len(vote_idx)):\\n                X_sub = X[:,:,vote_idx[i]:vote_idx[i]+200]\\n                output = model(X_sub)\\n                pred = torch.argmax(output, dim=1)\\n                if i == 0:\\n                    vote_matrix = np.asarray(pred.cpu().view(-1, 1))\\n                else:\\n                    vote_matrix = np.hstack((vote_matrix, np.asarray(pred.cpu().view(-1,1))))\\n                for row in range(y.shape[0]):\\n                    vote_pred[row] = np.bincount(vote_matrix[row, :]).argmax()\\n            vote_pred = torch.from_numpy(vote_pred).long()\\n            correct += torch.sum(vote_pred == y.cpu()).item()\\n            total += y.shape[0]\\n        test_acc = correct / total \\n    else:\\n        X_test, y_test, p_test = Aug_Data(X_test, y_test, p_test, aug_type=aug_type)\\n        EEG_testset = EEG_Dataset(X_test=X_test, y_test=y_test, p_test=p_test, mode=\\'test\\')\\n        EEG_testloader = DataLoader(EEG_testset, batch_size=128, shuffle=False)\\n        for idx, batch in enumerate(EEG_testloader):\\n            X = batch[\\'X\\'].to(device)\\n            y = batch[\\'y\\'].to(device)\\n            output = model(X)                    \\n            pred = torch.argmax(output, dim=1)\\n            correct += torch.sum(pred == y.cpu()).item()\\n            total += y.shape[0]\\n        test_acc = correct / total\\n    print (\\'Testing Accuracy: {:.4f}\\'.format(test_acc))\\n'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-S67grVAoSt",
        "colab_type": "code",
        "outputId": "2049f171-aae1-4efc-de73-75624bd932d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "########################################################\n",
        "\n",
        "# If running with Google Colab\n",
        "# Create a folder \"C247\" and then store the project datasets within that folder\n",
        "# Check that your datasets are setup correctly\n",
        "\n",
        "!ls \"/content/gdrive/My Drive/C247\" # File path"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "data_utils.py\t   person_test.npy\t   X_test.npy\t      y_train_valid.npy\n",
            "EEG_loading.ipynb  person_train_valid.npy  X_train_valid.npy\n",
            "models.py\t   utils.py\t\t   y_test.npy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-jkt1ZNZWIhi",
        "colab_type": "code",
        "outputId": "5c2e9be0-621a-4f89-dee4-8dbdf4c727fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from models import *\n",
        "from utils import *\n",
        "from data_utils import *"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU is available\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJfyBLNLA686",
        "colab_type": "code",
        "outputId": "9a5e5373-a47e-45e8-a114-9ec4d6ba570c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "# X_test = np.load(\"X_test.npy\")\n",
        "# y_test = np.load(\"y_test.npy\")\n",
        "# person_train_valid = np.load(\"person_train_valid.npy\")\n",
        "# X_train_valid = np.load(\"X_train_valid.npy\")\n",
        "# y_train_valid = np.load(\"y_train_valid.npy\")\n",
        "# person_test = np.load(\"person_test.npy\")\n",
        "\n",
        "# Change if your directory is different\n",
        "\n",
        "# dataset_path = './data/' # Yiming Path\n",
        "dataset_path = \"/content/gdrive/My Drive/C247/\" \n",
        "\n",
        "X_test = np.load(dataset_path + \"X_test.npy\")\n",
        "y_test = np.load(dataset_path + \"y_test.npy\")\n",
        "person_train_valid = np.load(dataset_path + \"person_train_valid.npy\")\n",
        "X_train_valid = np.load(dataset_path + \"X_train_valid.npy\")\n",
        "y_train_valid = np.load(dataset_path + \"y_train_valid.npy\")\n",
        "person_test = np.load(dataset_path + \"person_test.npy\")\n",
        "print ('Training/Valid data shape: {}'.format(X_train_valid.shape))\n",
        "print ('Test data shape: {}'.format(X_test.shape))\n",
        "print ('Training/Valid target shape: {}'.format(y_train_valid.shape))\n",
        "print ('Test target shape: {}'.format(y_test.shape))\n",
        "print ('Person train/valid shape: {}'.format(person_train_valid.shape))\n",
        "print ('Person test shape: {}'.format(person_test.shape))\n",
        "\n",
        "train_fold, val_fold = Train_Val_Data(X_train_valid, y_train_valid)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training/Valid data shape: (2115, 22, 1000)\n",
            "Test data shape: (443, 22, 1000)\n",
            "Training/Valid target shape: (2115,)\n",
            "Test target shape: (443,)\n",
            "Person train/valid shape: (2115, 1)\n",
            "Person test shape: (443, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90IvaY2LTyY1",
        "colab_type": "text"
      },
      "source": [
        "### K-Fold Training and Validation (use k=1 to get results faster)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3_bl5-h3DxE",
        "colab_type": "text"
      },
      "source": [
        "Data $\\rightarrow$ Window $\\rightarrow$ CWT"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QrIa4w_EqqOi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e6c0b50a-c5d5-4951-f15b-4f24139f88f0"
      },
      "source": [
        "# setting up the data augmentation here\n",
        "aug_type = 'cwt'\n",
        "\n",
        "#WINDOW PARAMS\n",
        "window_size = 200\n",
        "window_stride = 20\n",
        "vote_num = 20\n",
        "\n",
        "#STFT PARAMS\n",
        "stft_size = \n",
        "stft_stride = 5\n",
        "\n",
        "#CWT PARAMS\n",
        "cwt_level = 10\n",
        "cwt_scale = 3\n",
        "\n",
        "#RNN PARAMS\n",
        "rnn_type = 'GRU'\n",
        "dropout_param = 0.4\n",
        "num_epochs = 20\n",
        "hidden_size = 50\n",
        "lr = 1e-3\n",
        "weight_decay = 1e-3\n",
        "\n",
        "\n",
        "\n",
        "for k in range(1):\n",
        "    # indicate hyperparameters here\n",
        "    print ('fold {}'.format(k+1))\n",
        "    X_train, y_train, p_train = X_train_valid[train_fold[k]], y_train_valid[train_fold[k]], person_train_valid[train_fold[k]]\n",
        "    X_val, y_val, p_val = X_train_valid[val_fold[k]], y_train_valid[val_fold[k]], person_train_valid[val_fold[k]]\n",
        "\n",
        "    #apply window\n",
        "    _, X_train, y_train, p_train = window_data(X_train,y_train,p_train,window_size=window_size,stride=window_stride)\n",
        "    _, X_val, y_val, p_val = window_data(X_val,y_val,p_val,window_size=window_size,stride=window_stride)\n",
        "    print('Data windowed...')\n",
        "\n",
        "    #make data N(0,1)\n",
        "    mu = np.mean(X_train, axis=(0,2), keepdims = True) \n",
        "    std = np.std(X_train, axis=(0,2), keepdims = True)\n",
        "    X_train = standardize(X_train, mu, std)\n",
        "    X_val = standardize(X_val, mu, std)\n",
        "    \n",
        "    #augment data with requested augmentation\n",
        "    X_train, y_train, p_train = Aug_Data(X_train, y_train, p_train, aug_type=aug_type, window_size=window_size,cwt_level=cwt_level,cwt_scale=cwt_scale,stft_size=stft_size,stft_stride=stft_stride,window_stride=window_stride)\n",
        "    X_val, y_val, p_val = Aug_Data(X_val, y_val, p_val, window_size=window_size, aug_type=aug_type, cwt_level=cwt_level,cwt_scale=cwt_scale,stft_size=stft_size,stft_stride=stft_stride,window_stride=window_stride)\n",
        "    print('Transform applied...')\n",
        "    model, criterion, optimizer = InitRNN(rnn_type=rnn_type, dropout=dropout_param, input_size=X_train.shape[1],hidden_size=hidden_size, lr=lr,weight_decay=weight_decay)\n",
        "    EEG_trainset = EEG_Dataset(X_train=X_train, y_train=y_train, p_train=p_train, mode='train')\n",
        "    EEG_trainloader = DataLoader(EEG_trainset, batch_size=128, shuffle=True)\n",
        "    EEG_valset = EEG_Dataset(X_val=X_val, y_val=y_val, p_val=p_val, mode='val')\n",
        "    EEG_valloader = DataLoader(EEG_valset, batch_size=128, shuffle=False)\n",
        "    print('Data prepared, model initialized, beginning training...')\n",
        "    best_model = TrainValRNN(model, criterion, optimizer, EEG_trainloader, EEG_valloader, num_epochs=num_epochs, aug_type='window', window_size=window_size, vote_num=vote_num)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fold 1\n",
            "Data windowed...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5yGSJZ62TyY7",
        "colab_type": "text"
      },
      "source": [
        "### Training and Testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PT9IdrogTyY9",
        "colab_type": "code",
        "outputId": "c8009825-3c71-4acb-f6de-90a317e1b270",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "TestRNN(model, X_test, y_test, person_test, aug_type=aug_type, window_size=window_size, vote_num=vote_num)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing Accuracy: 0.4718\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2Ps1vtETyZC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_83ApEf9baF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tU0gQ8x_9blp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rH2XndIyBGzS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LoZ83SQ9brA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mz6yOUX99bos",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}