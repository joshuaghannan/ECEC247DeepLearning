{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.10-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "ee247",
   "display_name": "python3.6(ee247)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import scipy.signal as sig\n",
    "import pywt"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set up the Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "GPU is available\n"
    }
   ],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda:1\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Training/Valid data shape: (2115, 22, 1000)\nTest data shape: (443, 22, 1000)\nTraining/Valid target shape: (2115,)\nTest target shape: (443,)\nPerson train/valid shape: (2115, 1)\nPerson test shape: (443, 1)\n"
    }
   ],
   "source": [
    "dataset_path = './data/'\n",
    "X_train_valid = np.load(dataset_path + \"X_train_valid.npy\")\n",
    "y_train_valid = np.load(dataset_path + \"y_train_valid.npy\")\n",
    "person_train_valid = np.load(dataset_path + \"person_train_valid.npy\")\n",
    "X_test = np.load(dataset_path + \"X_test.npy\")\n",
    "y_test = np.load(dataset_path + \"y_test.npy\")\n",
    "person_test = np.load(dataset_path + \"person_test.npy\")\n",
    "print ('Training/Valid data shape: {}'.format(X_train_valid.shape))\n",
    "print ('Test data shape: {}'.format(X_test.shape))\n",
    "print ('Training/Valid target shape: {}'.format(y_train_valid.shape))\n",
    "print ('Test target shape: {}'.format(y_test.shape))\n",
    "print ('Person train/valid shape: {}'.format(person_train_valid.shape))\n",
    "print ('Person test shape: {}'.format(person_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### K-Fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some major changes here for the Train_Val_Data function\n",
    "def Train_Val_Data(X_train_valid, y_train_val):\n",
    "    '''\n",
    "    split the train_valid into k folds (we fix k = 5 here)\n",
    "    return: list of index of train data and val data of k folds\n",
    "    train_fold[i], val_fold[i] is the index for training and validation in the i-th fold \n",
    "\n",
    "    '''\n",
    "    fold_idx = []\n",
    "    train_fold = []\n",
    "    val_fold = []\n",
    "    train_val_num = X_train_valid.shape[0]\n",
    "    fold_num = int(train_val_num / 5)\n",
    "    perm = np.random.permutation(train_val_num)\n",
    "    for k in range(5):\n",
    "        fold_idx.append(np.arange(k*fold_num, (k+1)*fold_num, 1))\n",
    "    for k in range(5):\n",
    "        val_fold.append(fold_idx[k])\n",
    "        count = 0\n",
    "        for i in range(5):\n",
    "            if i != k:\n",
    "                if count == 0:\n",
    "                    train_idx = fold_idx[i]\n",
    "                else:\n",
    "                    train_idx = np.concatenate((train_idx, fold_idx[i]))\n",
    "                count += 1\n",
    "        train_fold.append(train_idx)\n",
    "\n",
    "    return train_fold, val_fold"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data Augmentation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1. Window Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def window_data(X, y, p, window_size, stride):\n",
    "  '''\n",
    "  X (a 3-d tensor) of size (#trials, #electrodes, #time series)\n",
    "  y (#trials,): label \n",
    "  p (#trials, 1): person id\n",
    "\n",
    "  X_new1: The first output stacks the windowed data in a new dimension, resulting \n",
    "    in a 4-d tensor of size (#trials x #electrodes x #windows x #window_size).\n",
    "  X_new2: The second option makes the windows into new trails, resulting in a new\n",
    "    X tensor of size (#trials*#windows x #electrodes x #window_size). To account \n",
    "    for the larger number of trials, we also need to augment the y data.\n",
    "  y_new: The augmented y vector of size (#trials*#windows) to match X_new2.\n",
    "  p_new: The augmented p vector of size (#trials*#windows) to match X_new2\n",
    " \n",
    "  '''\n",
    "  num_sub_trials = int((X.shape[2]-window_size)/stride)\n",
    "  X_new1 = np.empty([X.shape[0],X.shape[1],num_sub_trials,window_size])\n",
    "  X_new2 = np.empty([X.shape[0]*num_sub_trials,X.shape[1],window_size])\n",
    "  y_new = np.empty([X.shape[0]*num_sub_trials])\n",
    "  p_new = np.empty([X.shape[0]*num_sub_trials])\n",
    "  for i in range(X.shape[0]):\n",
    "    for j in range(X.shape[1]):\n",
    "      for k in range(num_sub_trials):\n",
    "        X_new1[i,j,k:k+window_size]    = X[i,j,k*stride:k*stride+window_size]\n",
    "        X_new2[i*num_sub_trials+k,j,:] = X[i,j,k*stride:k*stride+window_size]\n",
    "        y_new[i*num_sub_trials+k] = y[i]\n",
    "        p_new[i*num_sub_trials+k] = p[i]\n",
    "  return X_new1, X_new2, y_new, p_new"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2. STFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that computes the short-time fourier transform of the data and returns the spectrogram\n",
    "def stft_data(X, window, stride):\n",
    "    '''\n",
    "    Inputs:\n",
    "    X - input data, last dimension is one which transform will be taken across.\n",
    "    window - size of sliding window to take transform across\n",
    "    stride - stride of sliding window across time-series\n",
    "\n",
    "    Returns:\n",
    "    X_STFT - Output data, same shape as input with last dimension replaced with two new dimensions, F x T.\n",
    "            where F = window//2 + 1 is the frequency axis\n",
    "            and T = (input_length - window)//stride + 1, similar to the formula for aconvolutional filter.\n",
    "    t - the corresponding times for the time axis, T\n",
    "    f - the corresponding frequencies on the frequency axis, F.\n",
    "\n",
    "    reshape X_STFT (N, C, F, T) to (N, C*F, T) to fit the input of rnn\n",
    "\n",
    "    Note that a smaller window means only higher frequencies may be found, but give finer time resolution.\n",
    "    Conversely, a large window gives better frequency resolution, but poor time resolution.\n",
    "\n",
    "    '''\n",
    "    noverlap = window-stride\n",
    "    #print(noverlap)\n",
    "    if noverlap < 0 :\n",
    "        print('Stride results in skipped data!')\n",
    "        return\n",
    "    f, t, X_STFT = sig.spectrogram(X,nperseg=window,noverlap=noverlap,fs=250, return_onesided=True)\n",
    "    N, C, F, T = X_STFT.shape\n",
    "    X_STFT = X_STFT.reshape(N, C*F, T)\n",
    "    return X_STFT"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3. CWT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cwt_data(X, num_levels, top_scale=3):\n",
    "    '''\n",
    "    Takes in data, computes CWT using the mexican hat or ricker wavelet using scipy\n",
    "    Also takes in the top scale parameter.  I use logspace, so scale goes from 1 -> 2^top_scale with num_levels steps.\n",
    "    Appends to the data a new dimension, of size 'num_levels'\n",
    "    New dimension corresponds to wavelet content at num_levels different scalings (linear)\n",
    "    also returns the central frequencies that the scalings correspond to\n",
    "    input data is N x C X T\n",
    "    output data is N x C x T x F\n",
    "    note: CWT is fairly slow to compute\n",
    "\n",
    "    # EXAMPLE USAGE\n",
    "    test, freqs = cwt_data(X_train_valid[0:5,:,:],num_levels=75,top_scale=4)\n",
    "    '''\n",
    "    scales = np.logspace(start=0,stop=top_scale,num=num_levels)\n",
    "    out = np.empty((X.shape[0],X.shape[1],X.shape[2],num_levels))\n",
    "    for i in range(X.shape[0]):\n",
    "        for j in range(X.shape[1]):\n",
    "            coef = sig.cwt(X[i,j,:],sig.ricker,scales)\n",
    "            out[i,j,:] = coef.T\n",
    "    freqs = pywt.scale2frequency('mexh',scales)*250\n",
    "    N, C, T, F = out.shape\n",
    "    X_CWT = np.transpose(out, (0,1,3,2)).reshape(N, C*F, T)\n",
    "    return X_CWT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Aug_Data(X, y, p, aug_type=None, window_size=200, window_stride=20, stft_size=None, stft_stride=None, cwt_level=None, cwt_scale=None):\n",
    "    if aug_type == None:\n",
    "        X_aug, y_aug, p_aug = X, y, p\n",
    "    elif aug_type == \"window\":\n",
    "        _, X_aug, y_aug, p_aug = window_data(X, y, p, window_size, window_stride)\n",
    "    elif aug_type == \"stft\":\n",
    "        X_aug = stft_data(X, stft_size, stft_stride)\n",
    "        y_aug, p_aug = y, p\n",
    "    elif aug_type == 'cwt':\n",
    "        X_aug = cwt_data(X, cwt_level, cwt_scale)\n",
    "        y_aug, p_aug = y, p\n",
    "    \n",
    "    return X_aug, y_aug, p_aug"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Customized Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEG_Dataset(Dataset):\n",
    "    '''\n",
    "    use use fold_idx to instantiate different train val datasets for k-fold cross validation\n",
    "\n",
    "    '''\n",
    "    def __init__ (self, X_train=None, y_train=None, p_train=None, X_val=None, y_val=None, p_val=None, X_test=None, y_test=None, p_test=None, mode='train'):\n",
    "        if mode == 'train':\n",
    "            self.X = X_train\n",
    "            self.y = y_train- 769\n",
    "            self.p = p_train\n",
    "            \n",
    "        elif mode == 'val':\n",
    "            self.X = X_val\n",
    "            self.y = y_val- 769\n",
    "            self.p = p_val\n",
    "\n",
    "        elif mode == 'test':\n",
    "            self.X = X_test\n",
    "            self.y = y_test - 769        \n",
    "            self.p = p_test\n",
    "\n",
    "    def __len__(self):\n",
    "        return (self.X.shape[0])\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        '''\n",
    "        X: (augmented) time sequence \n",
    "        y: class label\n",
    "        p: person id\n",
    "\n",
    "        '''\n",
    "        X = torch.from_numpy(self.X[idx,:,:]).float()\n",
    "        y = torch.tensor(self.y[idx]).long()\n",
    "        p = torch.tensor(self.p[idx]).long()\n",
    "        #p = torch.from_numpy(self.p[idx,:]).long()     \n",
    "        sample = {'X': X, 'y': y, 'p':p}\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define Basic LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMnet(nn.Module):\n",
    "    '''\n",
    "    Create Basic LSTM:\n",
    "    2 layers\n",
    "\n",
    "    TODO: make number of layers, dropout, activation function, regularization all params\n",
    "    see ex: https://blog.floydhub.com/gru-with-pytorch/\n",
    "    '''\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_dim, dropout):\n",
    "        super(LSTMnet, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_dim = output_dim\n",
    "        self.rnn = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=2, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_dim)\n",
    "    \n",
    "    def forward(self, x, h=None):\n",
    "        x = x.permute(2, 0, 1)\n",
    "        if type(h) == type(None):\n",
    "            out, hn = self.rnn(x)\n",
    "        else:\n",
    "            out, hn = self.rnn(x, h.detach())\n",
    "        out = self.fc(out[-1, :, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define CNN + LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNLSTMnet(nn.Module):\n",
    "    '''\n",
    "    CNN + LSTM\n",
    "    \n",
    "    '''\n",
    "    def __init__(self, cnn_input_size, rnn_input_size, hidden_size, output_dim, dropout):\n",
    "        super(CNNLSTMnet, self).__init__()\n",
    "        self.cnn_input_size = cnn_input_size\n",
    "        self.rnn_input_size = rnn_input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_dim = output_dim\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(cnn_input_size, rnn_input_size, kernel_size=5, stride=2),\n",
    "            nn.BatchNorm1d(rnn_input_size),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        self.rnn = nn.LSTM(input_size=rnn_input_size, hidden_size=hidden_size, num_layers=2, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_dim)\n",
    "    \n",
    "    def forward(self, x, h=None):\n",
    "        out = self.cnn(x)\n",
    "        out = out.permute(2,0,1)\n",
    "        if type(h) == type(None):\n",
    "            out, hn = self.rnn(out)\n",
    "        else:\n",
    "            out, hn = self.rnn(out, h.detach())\n",
    "        out = self.fc(out[-1, :, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define Basic GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUnet(nn.Module):\n",
    "    '''\n",
    "    Create Basic GRU:\n",
    "    2 layers\n",
    "\n",
    "    TODO: make number of layers, dropout, activation function, regularization all params\n",
    "    see ex: https://blog.floydhub.com/gru-with-pytorch/\n",
    "    '''\n",
    "\n",
    "    def __init__(self, input_size, hidden_size, output_dim, dropout):\n",
    "        super(GRUnet, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_dim = output_dim\n",
    "        self.rnn = nn.GRU(input_size=input_size, hidden_size=hidden_size, num_layers=2, dropout=dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_dim)\n",
    "    \n",
    "    def forward(self, x, h=None):\n",
    "        x = x.permute(2, 0, 1)\n",
    "        if type(h) == type(None):\n",
    "            out, hn = self.rnn(x)\n",
    "        else:\n",
    "            out, hn = self.rnn(x, h.detach())\n",
    "        out = self.fc(out[-1, :, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### RNN Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def InitRNN(rnn_type=\"LSTM\", input_size=22, rnn_input_size=40, hidden_size=50, output_dim=4, dropout=0.5, lr=1e-3):\n",
    "    '''\n",
    "    Function to initialize RNN\n",
    "    \n",
    "    input: RNN type(LSTM, GRU), and other params if neccessary (regularization, acitvation, dropout, num layers, etc.)\n",
    "\n",
    "    output: model, criterion, optimizer\n",
    "\n",
    "    TODO: Eventually should also take in params such as dropout, number of layers, and activation function(s), etc.\n",
    "    '''\n",
    "\n",
    "    if rnn_type==\"LSTM\":\n",
    "        model = LSTMnet(input_size=input_size, hidden_size=hidden_size, output_dim=output_dim, dropout=dropout).to(device)\n",
    "\n",
    "    elif rnn_type==\"GRU\":\n",
    "        model = GRUnet(input_size=input_size, hidden_size=hidden_size, output_dim=output_dim, dropout=dropout).to(device)\n",
    "    \n",
    "    elif rnn_type==\"CNNLSTM\":\n",
    "        model = CNNLSTMnet(cnn_input_size=input_size, rnn_input_size=rnn_input_size, hidden_size=hidden_size, output_dim=output_dim, dropout=dropout).to(device)\n",
    "\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    return model, criterion, optimizer\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### K-Fold Training and Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainRNN(trainloader, valloader, num_epochs=20, verbose=True, aug_type=None):\n",
    "    val_acc_list = []\n",
    "    for ep in range(num_epochs):\n",
    "        tstart = time.time()\n",
    "        running_loss = 0.0\n",
    "        correct, total = 0, 0\n",
    "        for idx, batch in enumerate(EEG_trainloader):\n",
    "            optimizer.zero_grad()\n",
    "            X = batch['X'].to(device)\n",
    "            y = batch['y'].to(device)\n",
    "            output = model(X)\n",
    "            loss = criterion(output, y)\n",
    "            running_loss += loss.item()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            pred = torch.argmax(output, dim=1)\n",
    "            correct += torch.sum(pred == y).item()\n",
    "            total += y.shape[0]\n",
    "        train_acc = correct / total\n",
    "        train_loss = running_loss\n",
    "        print ('val...')\n",
    "        '''\n",
    "        The validation need to be customized according to the data augmenation type\n",
    "        for stft and cwt: they didn't increase the number of trials, we can directly pass the augmented data to the model\n",
    "        for window: it increase the number of trials, we need to do a voting for different subsequences in one trial\n",
    "        \n",
    "        '''\n",
    "        if aug_type == 'window':\n",
    "            correct, total = 0, 0\n",
    "            for idx, batch in enumerate(EEG_valloader):\n",
    "                #X = batch['X'].permute(2, 0, 1).to(device)\n",
    "                X = batch['X'].to(device)\n",
    "                y = batch['y'].to(device)\n",
    "                vote_idx = np.random.choice(1000-window_size, vote_num)\n",
    "                vote_pred = np.zeros(y.shape[0])\n",
    "                for i in range(len(vote_idx)):\n",
    "                    X_sub = X[:,:,vote_idx[i]:vote_idx[i]+window_size]\n",
    "                    output = model(X_sub)\n",
    "                    pred = torch.argmax(output, dim=1)\n",
    "                    if i == 0:\n",
    "                        vote_matrix = np.asarray(pred.cpu().view(-1, 1))\n",
    "                    else:\n",
    "                        vote_matrix = np.hstack((vote_matrix, np.asarray(pred.cpu().view(-1,1))))\n",
    "                for row in range(y.shape[0]):\n",
    "                    vote_pred[row] = np.bincount(vote_matrix[row, :]).argmax()\n",
    "                vote_pred = torch.from_numpy(vote_pred).long()\n",
    "                correct += torch.sum(vote_pred == y.cpu()).item()\n",
    "                total += y.shape[0]\n",
    "            val_acc = correct / total        \n",
    "        else:\n",
    "            correct, total = 0, 0\n",
    "            for idx, batch in enumerate(EEG_valloader):\n",
    "                X = batch['X'].to(device)\n",
    "                y = batch['y'].to(device)\n",
    "                output = model(X)                    \n",
    "                pred = torch.argmax(output, dim=1)\n",
    "                correct += torch.sum(pred == y.cpu()).item()\n",
    "                total += y.shape[0]\n",
    "            val_acc = correct / total\n",
    "        tend = time.time()\n",
    "        if verbose:\n",
    "            print('epoch: {:<3d}    time: {:<3.2f}    loss: {:<3.3f}    train acc: {:<1.3f}    val acc: {:<1.3f}'.format(ep+1, tend - tstart, train_loss, train_acc, val_acc))\n",
    "        val_acc_list.append(val_acc)\n",
    "    best_val_acc = max(val_acc_list)\n",
    "    return best_val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.Split the data to train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_fold, val_fold = Train_Val_Data(X_train_valid, y_train_valid)\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Initialize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# indicate hyperparameters here\n",
    "model, criterion, optimizer = InitRNN(rnn_type='CNNLSTM')\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Do K-Fold training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "fold 1\nval...\nepoch: 1      time: 17.48    loss: 727.555    train acc: 0.287    val acc: 0.253\nval...\nepoch: 2      time: 17.27    loss: 719.593    train acc: 0.313    val acc: 0.326\nval...\nepoch: 3      time: 17.84    loss: 706.714    train acc: 0.344    val acc: 0.407\nval...\nepoch: 4      time: 17.23    loss: 683.983    train acc: 0.382    val acc: 0.416\nval...\nepoch: 5      time: 17.26    loss: 660.820    train acc: 0.415    val acc: 0.437\nval...\nepoch: 6      time: 17.35    loss: 641.622    train acc: 0.436    val acc: 0.390\nval...\nepoch: 7      time: 17.39    loss: 619.953    train acc: 0.463    val acc: 0.463\nval...\nepoch: 8      time: 17.46    loss: 598.641    train acc: 0.492    val acc: 0.482\nval...\nepoch: 9      time: 17.56    loss: 577.860    train acc: 0.515    val acc: 0.397\nval...\nepoch: 10     time: 17.61    loss: 554.077    train acc: 0.540    val acc: 0.407\nval...\nepoch: 11     time: 17.42    loss: 533.734    train acc: 0.564    val acc: 0.435\nval...\nepoch: 12     time: 17.45    loss: 510.603    train acc: 0.586    val acc: 0.423\nval...\nepoch: 13     time: 17.39    loss: 490.346    train acc: 0.606    val acc: 0.418\nval...\nepoch: 14     time: 17.13    loss: 468.595    train acc: 0.627    val acc: 0.400\nval...\nepoch: 15     time: 17.23    loss: 448.266    train acc: 0.647    val acc: 0.482\nval...\n"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-5e53bf3d9669>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mEEG_valset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEEG_Dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mEEG_valloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEEG_valset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mbest_val_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mTrainRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEEG_trainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEEG_valloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maug_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maug_type\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'average best validation accuracy of 5 folds is :{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_val_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-22-c1b39af26ef8>\u001b[0m in \u001b[0;36mTrainRNN\u001b[0;34m(trainloader, valloader, num_epochs, verbose, aug_type)\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvote_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m                     \u001b[0mX_sub\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvote_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mvote_idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m                     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_sub\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m                     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ee247/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-26f8a34634fc>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, h)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ee247/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/ee247/lib/python3.6/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    557\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 559\u001b[0;31m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    560\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "aug_type = \"window\"\n",
    "window_size = 200\n",
    "vote_num = 20\n",
    "best_val_acc = 0.0\n",
    "for k in range(5):\n",
    "    # indicate hyperparameters here\n",
    "    model, criterion, optimizer = InitRNN(rnn_type='CNNLSTM')\n",
    "    print ('fold {}'.format(k+1))\n",
    "    X_train, y_train, p_train = X_train_valid[train_fold[k]], y_train_valid[train_fold[k]], person_train_valid[train_fold[k]]\n",
    "    X_val, y_val, p_val = X_train_valid[val_fold[k]], y_train_valid[val_fold[k]], person_train_valid[val_fold[k]]\n",
    "    X_train, y_train, p_train = Aug_Data(X_train, y_train, p_train, aug_type=aug_type)\n",
    "    if aug_type != 'window':\n",
    "        X_val, y_val, p_val = Aug_Data(X_val, y_val, p_val, aug_type=aug_type)\n",
    "    EEG_trainset = EEG_Dataset(X_train=X_train, y_train=y_train, p_train=p_train, mode='train')\n",
    "    EEG_trainloader = DataLoader(EEG_trainset, batch_size=128, shuffle=True)\n",
    "    EEG_valset = EEG_Dataset(X_val=X_val, y_val=y_val, p_val=p_val, mode='val')\n",
    "    EEG_valloader = DataLoader(EEG_valset, batch_size=128, shuffle=False)\n",
    "    best_val_acc += TrainRNN(EEG_trainloader, EEG_valloader, aug_type=aug_type) / 5\n",
    "print ('average best validation accuracy of 5 folds is :{}'.format(best_val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Testing Accuracy: 0.4898\n"
    }
   ],
   "source": [
    "X_test, y_test, p_test = X_test, y_test, person_test\n",
    "if aug_type == 'window':\n",
    "    EEG_testset = EEG_Dataset(X_train, y_train, p_train, X_val, y_val, p_val, X_test, y_test, p_test, mode='test')\n",
    "    EEG_testloader = DataLoader(EEG_testset, batch_size=128, shuffle=False)\n",
    "    correct, total = 0, 0\n",
    "    for idx, batch in enumerate(EEG_testloader):\n",
    "        X = batch['X'].to(device)\n",
    "        y = batch['y'].to(device)\n",
    "        vote_idx = np.random.choice(1000-window_size, vote_num)\n",
    "        vote_pred = np.zeros(y.shape[0])\n",
    "        for i in range(len(vote_idx)):\n",
    "            X_sub = X[:,:,vote_idx[i]:vote_idx[i]+200]\n",
    "            output = model(X_sub)\n",
    "            pred = torch.argmax(output, dim=1)\n",
    "            if i == 0:\n",
    "                vote_matrix = np.asarray(pred.cpu().view(-1, 1))\n",
    "            else:\n",
    "                vote_matrix = np.hstack((vote_matrix, np.asarray(pred.cpu().view(-1,1))))\n",
    "            for row in range(y.shape[0]):\n",
    "                vote_pred[row] = np.bincount(vote_matrix[row, :]).argmax()\n",
    "        vote_pred = torch.from_numpy(vote_pred).long()\n",
    "        correct += torch.sum(vote_pred == y.cpu()).item()\n",
    "        total += y.shape[0]\n",
    "    test_acc = correct / total \n",
    "else:\n",
    "    X_test, y_test, p_test = Aug_Data(X_test, y_test, p_test)\n",
    "    EEG_testset = EEG_Dataset(X_test=X_test, y_test=y_test, p_test=p_test, mode='test')\n",
    "    EEG_testloader = DataLoader(EEG_testset, batch_size=128, shuffle=False)    \n",
    "    correct, total = 0, 0\n",
    "    for idx, batch in enumerate(EEG_testloader):\n",
    "        X = batch['X'].to(device)\n",
    "        y = batch['y'].to(device)\n",
    "        output = model(X)                    \n",
    "        pred = torch.argmax(output, dim=1)\n",
    "        correct += torch.sum(pred == y.cpu()).item()\n",
    "        total += y.shape[0]\n",
    "    test_acc = correct / total\n",
    "print ('Testing Accuracy: {:.4f}'.format(test_acc))\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}