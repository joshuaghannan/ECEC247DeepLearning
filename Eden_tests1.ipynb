{"nbformat":4,"nbformat_minor":0,"metadata":{"orig_nbformat":2,"file_extension":".py","mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3,"kernelspec":{"name":"python3","display_name":"Python 3"},"colab":{"name":"Eden_tests1.ipynb","provenance":[],"collapsed_sections":["cLD4Nd_4DLFe","-DDfijHMPeas"],"toc_visible":true,"machine_shape":"hm"}},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github","colab_type":"text"},"source":["<a href=\"https://colab.research.google.com/github/joshuaghannan/ECEC247_Project/blob/master/Updated_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"A6BvVAE2tvB0","colab_type":"text"},"source":["#Setup"]},{"cell_type":"code","metadata":{"id":"ZsCG9DHzEI2B","colab_type":"code","colab":{}},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import time\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","import scipy.signal as sig\n","import pywt\n","from sklearn.decomposition import FastICA"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o80JLlxyEI2H","colab_type":"text"},"source":["### Set up the Device"]},{"cell_type":"code","metadata":{"id":"-HjZycAvEI2J","colab_type":"code","outputId":"82deae88-b080-49b4-f309-bf8a99231ed9","executionInfo":{"status":"ok","timestamp":1583957413563,"user_tz":420,"elapsed":7866,"user":{"displayName":"EDEN HANEY","photoUrl":"","userId":"04611419970615248408"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n","is_cuda = torch.cuda.is_available()\n","\n","# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n","if is_cuda:\n","    device = torch.device(\"cuda\")\n","    # device = torch.device(\"cuda:1\") # For Yiming \n","    print(\"GPU is available\")\n","else:\n","    device = torch.device(\"cpu\")\n","    print(\"GPU not available, CPU used\")"],"execution_count":27,"outputs":[{"output_type":"stream","text":["GPU not available, CPU used\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"26wpkKr9Em1E","colab_type":"text"},"source":["### If Using Colab"]},{"cell_type":"code","metadata":{"id":"KQwpQEszAQ9u","colab_type":"code","outputId":"e93948c0-3258-4181-e9a3-741d9c684ceb","executionInfo":{"status":"ok","timestamp":1583957413565,"user_tz":420,"elapsed":7845,"user":{"displayName":"EDEN HANEY","photoUrl":"","userId":"04611419970615248408"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["########################################################\n","\n","# If running with Google Colab\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":28,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"E-S67grVAoSt","colab_type":"code","outputId":"8990e823-81b6-4156-df79-57f8ee6c0cf9","executionInfo":{"status":"ok","timestamp":1583957418608,"user_tz":420,"elapsed":12866,"user":{"displayName":"EDEN HANEY","photoUrl":"","userId":"04611419970615248408"}},"colab":{"base_uri":"https://localhost:8080/","height":86}},"source":["########################################################\n","\n","# If running with Google Colab\n","# Create a folder \"C247\" and then store the project datasets within that folder\n","# Check that your datasets are setup correctly\n","\n","!ls \"/content/gdrive/My Drive/C247\" # File path"],"execution_count":29,"outputs":[{"output_type":"stream","text":["EEG_loading.ipynb  person_train_valid.npy  X_train_valid.npy\n","FinalProject\t   __pycache__\t\t   y_test.npy\n","icasso.py\t   Updated_pipeline.ipynb  y_train_valid.npy\n","person_test.npy    X_test.npy\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"NiE2fkEzA3VO","colab_type":"text"},"source":["### Load the Datasets"]},{"cell_type":"code","metadata":{"id":"TJfyBLNLA686","colab_type":"code","outputId":"be02fefc-32c6-4d6c-c7fd-4be6a395e758","executionInfo":{"status":"ok","timestamp":1583957419271,"user_tz":420,"elapsed":13505,"user":{"displayName":"EDEN HANEY","photoUrl":"","userId":"04611419970615248408"}},"colab":{"base_uri":"https://localhost:8080/","height":121}},"source":["# X_test = np.load(\"X_test.npy\")\n","# y_test = np.load(\"y_test.npy\")\n","# person_train_valid = np.load(\"person_train_valid.npy\")\n","# X_train_valid = np.load(\"X_train_valid.npy\")\n","# y_train_valid = np.load(\"y_train_valid.npy\")\n","# person_test = np.load(\"person_test.npy\")\n","\n","# Change if your directory is different\n","\n","# dataset_path = './data/' # Yiming Path\n","dataset_path = \"/content/gdrive/My Drive/C247/\" \n","\n","X_test = np.load(dataset_path + \"X_test.npy\")\n","y_test = np.load(dataset_path + \"y_test.npy\")\n","person_train_valid = np.load(dataset_path + \"person_train_valid.npy\")\n","X_train_valid = np.load(dataset_path + \"X_train_valid.npy\")\n","y_train_valid = np.load(dataset_path + \"y_train_valid.npy\")\n","person_test = np.load(dataset_path + \"person_test.npy\")\n","print ('Training/Valid data shape: {}'.format(X_train_valid.shape))\n","print ('Test data shape: {}'.format(X_test.shape))\n","print ('Training/Valid target shape: {}'.format(y_train_valid.shape))\n","print ('Test target shape: {}'.format(y_test.shape))\n","print ('Person train/valid shape: {}'.format(person_train_valid.shape))\n","print ('Person test shape: {}'.format(person_test.shape))"],"execution_count":30,"outputs":[{"output_type":"stream","text":["Training/Valid data shape: (2115, 22, 1000)\n","Test data shape: (443, 22, 1000)\n","Training/Valid target shape: (2115,)\n","Test target shape: (443,)\n","Person train/valid shape: (2115, 1)\n","Person test shape: (443, 1)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"gQEPDT85uAON","colab_type":"text"},"source":["# Data Manipulation"]},{"cell_type":"markdown","metadata":{"id":"JzSs4ByOEI2V","colab_type":"text"},"source":["### K-Fold"]},{"cell_type":"code","metadata":{"id":"LNKfnV6iEI2W","colab_type":"code","colab":{}},"source":["# some major changes here for the Train_Val_Data function\n","def Train_Val_Data(X_train_valid, y_train_val):\n","    '''\n","    split the train_valid into k folds (we fix k = 5 here)\n","    return: list of index of train data and val data of k folds\n","    train_fold[i], val_fold[i] is the index for training and validation in the i-th fold \n","\n","    '''\n","    fold_idx = []\n","    train_fold = []\n","    val_fold = []\n","    train_val_num = X_train_valid.shape[0]\n","    fold_num = int(train_val_num / 5)\n","    perm = np.random.permutation(train_val_num)\n","    for k in range(5):\n","        fold_idx.append(np.arange(k*fold_num, (k+1)*fold_num, 1))\n","    for k in range(5):\n","        val_fold.append(fold_idx[k])\n","        count = 0\n","        for i in range(5):\n","            if i != k:\n","                if count == 0:\n","                    train_idx = fold_idx[i]\n","                else:\n","                    train_idx = np.concatenate((train_idx, fold_idx[i]))\n","                count += 1\n","        train_fold.append(train_idx)\n","\n","    return train_fold, val_fold"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UuWOwEaeEI23","colab_type":"text"},"source":["### Customized Dataset"]},{"cell_type":"code","metadata":{"id":"b-Kbua1ZEI28","colab_type":"code","colab":{}},"source":["class EEG_Dataset(Dataset):\n","    '''\n","    use use fold_idx to instantiate different train val datasets for k-fold cross validation\n","\n","    '''\n","    def __init__ (self, X_train=None, y_train=None, p_train=None, X_val=None, y_val=None, p_val=None, X_test=None, y_test=None, p_test=None, mode='train'):\n","        if mode == 'train':\n","            self.X = X_train\n","            self.y = y_train- 769\n","            self.p = p_train\n","            \n","        elif mode == 'val':\n","            self.X = X_val\n","            self.y = y_val- 769\n","            self.p = p_val\n","\n","        elif mode == 'test':\n","            self.X = X_test\n","            self.y = y_test - 769        \n","            self.p = p_test\n","\n","    def __len__(self):\n","        return (self.X.shape[0])\n","    \n","    def __getitem__(self, idx):\n","        '''\n","        X: (augmented) time sequence \n","        y: class label\n","        p: person id\n","\n","        '''\n","        X = torch.from_numpy(self.X[idx,:,:]).float()\n","        y = torch.tensor(self.y[idx]).long()\n","        p = torch.tensor(self.p[idx]).long()\n","        #p = torch.from_numpy(self.p[idx,:]).long()     \n","        sample = {'X': X, 'y': y, 'p':p}\n","\n","        return sample"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hbiGv8ouEI2b","colab_type":"text"},"source":["## Data Augmentation Functions"]},{"cell_type":"markdown","metadata":{"id":"3iVOP_rJtq1X","colab_type":"text"},"source":["###Center and Whiten Data\n","Scales and shifts data to have zero mean and variance 1"]},{"cell_type":"code","metadata":{"id":"X4cxUmsptofA","colab_type":"code","colab":{}},"source":["from sklearn import preprocessing\n","def scale_data(X):\n","  #Takes 3-dim X and outputs scaled and shifted X_new with zero mean and var 1\n","  X_scaled = np.empty_like(X)\n","  for i in range(X.shape[1]):\n","    X_scaled[:,i,:] = preprocessing.scale(X[:,i,:])\n","  return X_scaled"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P_y4pb5vEI2d","colab_type":"text"},"source":["### 1. Window Data"]},{"cell_type":"code","metadata":{"id":"T5dPzOfbEI2e","colab_type":"code","colab":{}},"source":["def window_data(X, y, p, window_size, stride):\n","  '''\n","  X (a 3-d tensor) of size (#trials, #electrodes, #time series)\n","  y (#trials,): label \n","  p (#trials, 1): person id\n","\n","  X_new1: The first output stacks the windowed data in a new dimension, resulting \n","    in a 4-d tensor of size (#trials x #electrodes x #windows x #window_size).\n","  X_new2: The second option makes the windows into new trails, resulting in a new\n","    X tensor of size (#trials*#windows x #electrodes x #window_size). To account \n","    for the larger number of trials, we also need to augment the y data.\n","  y_new: The augmented y vector of size (#trials*#windows) to match X_new2.\n","  p_new: The augmented p vector of size (#trials*#windows) to match X_new2\n"," \n","  '''\n","  num_sub_trials = int((X.shape[2]-window_size)/stride)\n","  X_new1 = np.empty([X.shape[0],X.shape[1],num_sub_trials,window_size])\n","  X_new2 = np.empty([X.shape[0]*num_sub_trials,X.shape[1],window_size])\n","  y_new = np.empty([X.shape[0]*num_sub_trials])\n","  p_new = np.empty([X.shape[0]*num_sub_trials])\n","  for i in range(X.shape[0]):\n","    for j in range(X.shape[1]):\n","      for k in range(num_sub_trials):\n","        X_new1[i,j,k:k+window_size]    = X[i,j,k*stride:k*stride+window_size]\n","        X_new2[i*num_sub_trials+k,j,:] = X[i,j,k*stride:k*stride+window_size]\n","        y_new[i*num_sub_trials+k] = y[i]\n","        p_new[i*num_sub_trials+k] = p[i]\n","  N, C, NT, T = X_new1.shape\n","  X_new1 = (X_new1.reshape(N, C*NT, T))\n","  return X_new1, X_new2, y_new, p_new"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oQ7mRTjzEI2m","colab_type":"text"},"source":["### 2. STFT"]},{"cell_type":"code","metadata":{"id":"RU7uZckAEI2n","colab_type":"code","colab":{}},"source":["# Function that computes the short-time fourier transform of the data and returns the spectrogram\n","def stft_data(X, window, stride):\n","    '''\n","    Inputs:\n","    X - input data, last dimension is one which transform will be taken across.\n","    window - size of sliding window to take transform across\n","    stride - stride of sliding window across time-series\n","\n","    Returns:\n","    X_STFT - Output data, same shape as input with last dimension replaced with two new dimensions, F x T.\n","            where F = window//2 + 1 is the frequency axis\n","            and T = (input_length - window)//stride + 1, similar to the formula for aconvolutional filter.\n","    t - the corresponding times for the time axis, T\n","    f - the corresponding frequencies on the frequency axis, F.\n","\n","    reshape X_STFT (N, C, F, T) to (N, C*F, T) to fit the input of rnn\n","\n","    Note that a smaller window means only higher frequencies may be found, but give finer time resolution.\n","    Conversely, a large window gives better frequency resolution, but poor time resolution.\n","\n","    '''\n","    noverlap = window-stride\n","    #print(noverlap)\n","    if noverlap < 0 :\n","        print('Stride results in skipped data!')\n","        return\n","    f, t, X_STFT = sig.spectrogram(X,nperseg=window,noverlap=noverlap,fs=250, return_onesided=True)\n","    N, C, F, T = X_STFT.shape\n","    X_STFT = X_STFT.reshape(N, C*F, T)\n","    return X_STFT"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oKn0SF9dEI2s","colab_type":"text"},"source":["### 3. CWT"]},{"cell_type":"code","metadata":{"id":"HCxP0HCQEI2t","colab_type":"code","colab":{}},"source":["def cwt_data(X, num_levels, top_scale=3):\n","    '''\n","    Takes in data, computes CWT using the mexican hat or ricker wavelet using scipy\n","    Also takes in the top scale parameter.  I use logspace, so scale goes from 1 -> 2^top_scale with num_levels steps.\n","    Appends to the data a new dimension, of size 'num_levels'\n","    New dimension corresponds to wavelet content at num_levels different scalings (linear)\n","    also returns the central frequencies that the scalings correspond to\n","    input data is N x C X T\n","    output data is N x C x T x F\n","    note: CWT is fairly slow to compute\n","\n","    # EXAMPLE USAGE\n","    test, freqs = cwt_data(X_train_valid[0:5,:,:],num_levels=75,top_scale=4)\n","    '''\n","    scales = np.logspace(start=0,stop=top_scale,num=num_levels)\n","    out = np.empty((X.shape[0],X.shape[1],X.shape[2],num_levels))\n","    for i in range(X.shape[0]):\n","        for j in range(X.shape[1]):\n","            coef = sig.cwt(X[i,j,:],sig.ricker,scales)\n","            out[i,j,:] = coef.T\n","    freqs = pywt.scale2frequency('mexh',scales)*250\n","    N, C, T, F = out.shape\n","    X_CWT = np.transpose(out, (0,1,3,2)).reshape(N, C*F, T)\n","    return X_CWT"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"IrbdxGhi7fly","colab_type":"code","colab":{}},"source":["def cwt_data2(X, y, p, num_levels, top_scale=3):\n","    '''\n","    Takes in data, computes CWT using the mexican hat or ricker wavelet using scipy\n","    Also takes in the top scale parameter.  I use logspace, so scale goes from 1 -> 2^top_scale with num_levels steps.\n","    Appends to the data a new dimension, of size 'num_levels'\n","    New dimension corresponds to wavelet content at num_levels different scalings (linear)\n","    also returns the central frequencies that the scalings correspond to\n","    input data is N x C X T\n","    output data is N x C x T x F\n","    note: CWT is fairly slow to compute\n","\n","    # EXAMPLE USAGE\n","    test, freqs = cwt_data(X_train_valid[0:5,:,:],num_levels=75,top_scale=4)\n","    '''\n","    scales = np.logspace(start=0,stop=top_scale,num=num_levels)\n","    out = np.empty((X.shape[0],X.shape[1],X.shape[2],num_levels))\n","    for i in range(X.shape[0]):\n","        for j in range(X.shape[1]):\n","            coef = sig.cwt(X[i,j,:],sig.ricker,scales)\n","            out[i,j,:] = coef.T\n","    freqs = pywt.scale2frequency('mexh',scales)*250\n","    N, C, T, F = out.shape\n","    X_cwt = np.transpose(out, (0,3,1,2)).reshape(N*F, C, T)\n","    y_cwt = np.empty([X.shape[0]*F])\n","    p_cwt = np.empty([X.shape[0]*F])\n","    for i in range(X.shape[0]):\n","      for k in range(F):\n","        y_cwt[i*F+k] = y[i]\n","        p_cwt[i*F+k] = p[i]\n","    return X_cwt, y_cwt, p_cwt, F"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cLD4Nd_4DLFe","colab_type":"text"},"source":["### 4. Independent Component Analysis (ICA)"]},{"cell_type":"code","metadata":{"id":"8NBd6c-3DOy2","colab_type":"code","colab":{}},"source":["# FUNCTION TO COMPUTE THE ICA OF DATA\n","def ica_data(X, n_components):\n","  out = np.empty((X.shape[0], n_components, X.shape[-1]))\n","  for i in range(X.shape[0]):\n","    X_ica = FastICA(n_components=n_components, algorithm='deflation', whiten=True, max_iter=1000, tol=0.01)\n","    out[i,:,:] = X_ica.fit_transform(X[i,:,:].T).T\n","    #print(out.shape)  # Reconstruct signals\n","  return out"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LU9E1jC7_hdJ","colab_type":"text"},"source":["## Define data augmentation wrapper"]},{"cell_type":"code","metadata":{"id":"zSFCOa05EI2y","colab_type":"code","colab":{}},"source":["def Aug_Data(X, y, p, aug_type=None, window_size=200, window_stride=20, stft_size=None, stft_stride=None, cwt_level=None, cwt_scale=None, ica_num=None):\n","    F = 0\n","    if aug_type == None:\n","        X_aug, y_aug, p_aug = X, y, p\n","    elif aug_type == \"window\":\n","        _, X_aug, y_aug, p_aug = window_data(X, y, p, window_size, window_stride)\n","    elif aug_type == \"stft\":\n","        X_aug = stft_data(X, stft_size, stft_stride)\n","        y_aug, p_aug = y, p\n","    elif aug_type == 'cwt':\n","        X_aug = cwt_data(X, cwt_level, cwt_scale)\n","        y_aug, p_aug = y, p\n","    elif aug_type == 'cwt2':\n","        X_aug, y_aug, p_aug, F = cwt_data2(X, y, p, cwt_level, cwt_scale)\n","    elif aug_type == 'ica':\n","        X_aug = ica_data(X, ica_num)\n","        y_aug, p_aug = y, p\n","    \n","    return X_aug, y_aug, p_aug, F"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jOKsX1Xm-O42","colab_type":"text"},"source":["# Architectures"]},{"cell_type":"markdown","metadata":{"id":"pTAZw5n7EI3B","colab_type":"text"},"source":["### Define Basic LSTM"]},{"cell_type":"code","metadata":{"id":"2-drfGKkEI3D","colab_type":"code","colab":{}},"source":["class LSTMnet(nn.Module):\n","    '''\n","    Create Basic LSTM:\n","    2 layers\n","\n","    TODO: make number of layers, dropout, activation function, regularization all params\n","    see ex: https://blog.floydhub.com/gru-with-pytorch/\n","    '''\n","\n","    def __init__(self, input_size, hidden_size, output_dim, dropout):\n","        super(LSTMnet, self).__init__()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.output_dim = output_dim\n","        self.rnn = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=2, dropout=dropout)\n","        self.fc = nn.Linear(hidden_size, output_dim)\n","    \n","    def forward(self, x, h=None):\n","        if type(h) == type(None):\n","            out, hn = self.rnn(x)\n","        else:\n","            out, hn = self.rnn(x, h.detach())\n","        out = self.fc(out[-1, :, :])\n","        return out"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BNUej_RTEI3H","colab_type":"text"},"source":["### Define Basic GRU"]},{"cell_type":"code","metadata":{"id":"G48clZrtEI3J","colab_type":"code","colab":{}},"source":["class GRUnet(nn.Module):\n","    '''\n","    Create Basic GRU:\n","    2 layers\n","\n","    TODO: make number of layers, dropout, activation function, regularization all params\n","    see ex: https://blog.floydhub.com/gru-with-pytorch/\n","    '''\n","\n","    def __init__(self, input_size, hidden_size, output_dim, dropout):\n","        super(GRUnet, self).__init__()\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.output_dim = output_dim\n","        self.rnn = nn.GRU(input_size=input_size, hidden_size=hidden_size, num_layers=2, dropout=dropout)\n","        self.fc = nn.Linear(hidden_size, output_dim)\n","    \n","    def forward(self, x, h=None):\n","        if type(h) == type(None):\n","            out, hn = self.rnn(x)\n","        else:\n","            out, hn = self.rnn(x, h.detach())\n","        out = self.fc(out[-1, :, :])\n","        return out"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0-u6gCZLEI3P","colab_type":"text"},"source":["# RNN Initialization"]},{"cell_type":"code","metadata":{"id":"e-90NzPjEI3R","colab_type":"code","colab":{}},"source":["def InitRNN(rnn_type=\"LSTM\", input_size=22, hidden_size=50, output_dim=4, dropout=0.5, lr=1e-3):\n","    '''\n","    Function to initialize RNN\n","    \n","    input: RNN type(LSTM, GRU), and other params if neccessary (regularization, acitvation, dropout, num layers, etc.)\n","\n","    output: model, criterion, optimizer\n","\n","    TODO: Eventually should also take in params such as dropout, number of layers, and activation function(s), etc.\n","    '''\n","\n","    if rnn_type==\"LSTM\":\n","        model = LSTMnet(input_size=input_size, hidden_size=hidden_size, output_dim=output_dim, dropout=dropout).to(device)\n","\n","    elif rnn_type==\"GRU\":\n","        model = GRUnet(input_size=input_size, hidden_size=hidden_size, output_dim=output_dim, dropout=dropout).to(device)\n","\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","\n","    return model, criterion, optimizer\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vJwC6_vOEI3Y","colab_type":"text"},"source":["### K-Fold Training and Cross Validation"]},{"cell_type":"code","metadata":{"id":"36PDvdn1EI3g","colab_type":"code","colab":{}},"source":["def TrainRNN(trainloader, valloader, num_epochs=20, verbose=True, aug_type=None):\n","    val_acc_list = []\n","    for ep in range(num_epochs):\n","        tstart = time.time()\n","        running_loss = 0.0\n","        correct, total = 0, 0\n","        for idx, batch in enumerate(EEG_trainloader):\n","            optimizer.zero_grad()\n","            X = batch['X'].permute(2, 0, 1).to(device)\n","            y = batch['y'].to(device)\n","            output = model(X)\n","            loss = criterion(output, y)\n","            running_loss += loss.item()\n","            loss.backward()\n","            optimizer.step()\n","            pred = torch.argmax(output, dim=1)\n","            correct += torch.sum(pred == y).item()\n","            total += y.shape[0]\n","        train_acc = correct / total\n","        train_loss = running_loss\n","        '''\n","        The validation need to be customized according to the data augmenation type\n","        for stft and cwt: they didn't increase the number of trials, we can directly pass the augmented data to the model\n","        for window: it increase the number of trials, we need to do a voting for different subsequences in one trial\n","        \n","        '''\n","        if aug_type == 'window':\n","            correct, total = 0, 0\n","            for idx, batch in enumerate(EEG_valloader):\n","                X = batch['X'].permute(2, 0, 1).to(device)\n","                y = batch['y'].to(device)\n","                vote_idx = np.random.choice(1000-window_size, vote_num)\n","                vote_pred = np.zeros(y.shape[0])\n","                for i in range(len(vote_idx)):\n","                    X_sub = X[vote_idx[i]:vote_idx[i]+window_size,:,:]\n","                    output = model(X_sub)\n","                    pred = torch.argmax(output, dim=1)\n","                    if i == 0:\n","                        vote_matrix = np.asarray(pred.cpu().view(-1, 1))\n","                    else:\n","                        vote_matrix = np.hstack((vote_matrix, np.asarray(pred.cpu().view(-1,1))))\n","                    for row in range(y.shape[0]):\n","                        vote_pred[row] = np.bincount(vote_matrix[row, :]).argmax()\n","                vote_pred = torch.from_numpy(vote_pred).long()\n","                correct += torch.sum(vote_pred == y.cpu()).item()\n","                total += y.shape[0]\n","            val_acc = correct / total \n","        elif aug_type == 'cwt2':\n","            correct, total = 0, 0\n","            for idx, batch in enumerate(EEG_valloader):\n","                X = batch['X'].permute(2, 0, 1).to(device)\n","                y = batch['y'].to(device)\n","                vote_idx = np.random.choice(1000-window_size, vote_num)\n","                vote_pred = np.zeros(y.shape[0])\n","                for i in range(len(vote_idx)):\n","                    X_sub = X[vote_idx[i]:vote_idx[i]+window_size,:,:]\n","                    output = model(X_sub)\n","                    pred = torch.argmax(output, dim=1)\n","                    if i == 0:\n","                        vote_matrix = np.asarray(pred.cpu().view(-1, 1))\n","                    else:\n","                        vote_matrix = np.hstack((vote_matrix, np.asarray(pred.cpu().view(-1,1))))\n","                    for row in range(y.shape[0]):\n","                        vote_pred[row] = np.bincount(vote_matrix[row, :]).argmax()\n","                vote_pred = torch.from_numpy(vote_pred).long()\n","                correct += torch.sum(vote_pred == y.cpu()).item()\n","                total += y.shape[0]\n","            val_acc = correct / total        \n","        else:\n","            correct, total = 0, 0\n","            for idx, batch in enumerate(EEG_valloader):\n","                X = batch['X'].permute(2, 0, 1).to(device)\n","                y = batch['y'].to(device)\n","                output = model(X)                    \n","                pred = torch.argmax(output, dim=1)\n","                correct += torch.sum(pred == y.cpu()).item()\n","                total += y.shape[0]\n","            val_acc = correct / total\n","        tend = time.time()\n","        if verbose:\n","            print('epoch: {:<3d}    time: {:<3.2f}    loss: {:<3.3f}    train acc: {:<1.3f}    val acc: {:<1.3f}'.format(ep+1, tend - tstart, train_loss, train_acc, val_acc))\n","        val_acc_list.append(val_acc)\n","    best_val_acc = max(val_acc_list)\n","    return best_val_acc"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YyEi2-f-EI3u","colab_type":"text"},"source":["# Pipeline"]},{"cell_type":"markdown","metadata":{"id":"Y_dtSFRAEI36","colab_type":"text"},"source":["## 2. Initialize the model"]},{"cell_type":"code","metadata":{"id":"CLP8d7J3EI37","colab_type":"code","colab":{}},"source":["# indicate hyperparameters here\n","model, criterion, optimizer = InitRNN(rnn_type='LSTM')\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UMVxd_6nFVyQ","colab_type":"text"},"source":["# Experiments"]},{"cell_type":"markdown","metadata":{"id":"XeStPdgKEI4A","colab_type":"text"},"source":["##(small) windowed augmentation\n","Done with small number of data points below\n","\n","Testing Accuracy: 0.2596"]},{"cell_type":"markdown","metadata":{"id":"mdrFodRqEI3v","colab_type":"text"},"source":["#### Split the data to train and validation"]},{"cell_type":"code","metadata":{"id":"FBPsXoMTEI3x","colab_type":"code","outputId":"9ef31526-4f5f-42a7-cf0c-703924d47c99","executionInfo":{"status":"ok","timestamp":1583885618668,"user_tz":420,"elapsed":1901,"user":{"displayName":"EDEN HANEY","photoUrl":"","userId":"04611419970615248408"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["train_fold, val_fold = Train_Val_Data(X_train_valid, y_train_valid)\n","X_train_valid[train_fold[0]].shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1692, 22, 1000)"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"-Us-hqlvGhFW","colab_type":"text"},"source":["#### Run the thing"]},{"cell_type":"code","metadata":{"id":"mpaoajnlEI4B","colab_type":"code","outputId":"73edd747-b95b-4cfc-c530-8bc4d3522333","executionInfo":{"status":"ok","timestamp":1583873883305,"user_tz":420,"elapsed":545791,"user":{"displayName":"EDEN HANEY","photoUrl":"","userId":"04611419970615248408"}},"colab":{"base_uri":"https://localhost:8080/","height":190}},"source":["aug_type = 'window'\n","window_size = 80\n","vote_num = 8\n","best_val_acc = 0.0\n","model, criterion, optimizer = InitRNN(rnn_type='LSTM')\n","for k in range(1):\n","    # indicate hyperparameters here\n","    print ('fold {}'.format(k+1))\n","    X_train, y_train, p_train = X_train_valid[train_fold[k]], y_train_valid[train_fold[k]], person_train_valid[train_fold[k]]\n","    X_val, y_val, p_val = X_train_valid[val_fold[k]], y_train_valid[val_fold[k]], person_train_valid[val_fold[k]]\n","    X_train, y_train, p_train = Aug_Data(X_train[0:500,:,:], y_train[0:500], p_train[0:500], aug_type=aug_type, window_size=window_size, window_stride=vote_num)\n","    if aug_type != 'window':\n","        X_val, y_val, p_val = Aug_Data(X_val, y_val, p_val, aug_type=aug_type, window_size=window_size, window_stride=vote_num)\n","    EEG_trainset = EEG_Dataset(X_train=X_train, y_train=y_train, p_train=p_train, mode='train')\n","    EEG_trainloader = DataLoader(EEG_trainset, batch_size=128, shuffle=True)\n","    EEG_valset = EEG_Dataset(X_val=X_val, y_val=y_val, p_val=p_val, mode='val')\n","    EEG_valloader = DataLoader(EEG_valset, batch_size=128, shuffle=False)\n","    best_val_acc += TrainRNN(EEG_trainloader, EEG_valloader, aug_type=aug_type) / 5\n","print ('average best validation accuracy of 5 folds is :{}'.format(best_val_acc))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["fold 1\n","epoch: 1      time: 66.28    loss: 622.894    train acc: 0.267    val acc: 0.293\n","epoch: 2      time: 67.62    loss: 619.429    train acc: 0.292    val acc: 0.300\n","epoch: 3      time: 68.17    loss: 616.077    train acc: 0.306    val acc: 0.286\n","epoch: 4      time: 66.82    loss: 610.361    train acc: 0.324    val acc: 0.310\n","epoch: 5      time: 67.86    loss: 603.700    train acc: 0.346    val acc: 0.352\n","epoch: 6      time: 67.16    loss: 589.628    train acc: 0.375    val acc: 0.324\n","epoch: 7      time: 68.12    loss: 579.751    train acc: 0.392    val acc: 0.364\n","epoch: 8      time: 67.76    loss: 560.618    train acc: 0.427    val acc: 0.322\n","average best validation accuracy of 5 folds is :0.07281323877068559\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"d8dOg38mEI4G","colab_type":"code","outputId":"8b4c75a6-bd17-477e-e442-408c7f3ab69f","executionInfo":{"status":"ok","timestamp":1583873309410,"user_tz":420,"elapsed":4440,"user":{"displayName":"EDEN HANEY","photoUrl":"","userId":"04611419970615248408"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["X_test, y_test, p_test = X_test, y_test, person_test\n","if aug_type == 'window':\n","    EEG_testset = EEG_Dataset(X_train, y_train, p_train, X_val, y_val, p_val, X_test, y_test, p_test, mode='test')\n","    EEG_testloader = DataLoader(EEG_testset, batch_size=128, shuffle=False)\n","    correct, total = 0, 0\n","    for idx, batch in enumerate(EEG_testloader):\n","        X = batch['X'].permute(2, 0, 1).to(device)\n","        y = batch['y'].to(device)\n","        vote_idx = np.random.choice(1000-window_size, vote_num)\n","        vote_pred = np.zeros(y.shape[0])\n","        for i in range(len(vote_idx)):\n","            X_sub = X[vote_idx[i]:vote_idx[i]+200,:,:]\n","            output = model(X_sub)\n","            pred = torch.argmax(output, dim=1)\n","            if i == 0:\n","                vote_matrix = np.asarray(pred.cpu().view(-1, 1))\n","            else:\n","                vote_matrix = np.hstack((vote_matrix, np.asarray(pred.cpu().view(-1,1))))\n","            for row in range(y.shape[0]):\n","                vote_pred[row] = np.bincount(vote_matrix[row, :]).argmax()\n","        vote_pred = torch.from_numpy(vote_pred).long()\n","        correct += torch.sum(vote_pred == y.cpu()).item()\n","        total += y.shape[0]\n","    test_acc = correct / total \n","else:\n","    X_test, y_test, p_test = Aug_Data(X_test, y_test, p_test)\n","    EEG_testset = EEG_Dataset(X_test=X_test, y_test=y_test, p_test=p_test, mode='test')\n","    EEG_testloader = DataLoader(EEG_testset, batch_size=128, shuffle=False)    \n","    correct, total = 0, 0\n","    for idx, batch in enumerate(EEG_testloader):\n","        X = batch['X'].permute(2, 0, 1).to(device)\n","        y = batch['y'].to(device)\n","        output = model(X)                    \n","        pred = torch.argmax(output, dim=1)\n","        correct += torch.sum(pred == y.cpu()).item()\n","        total += y.shape[0]\n","    test_acc = correct / total\n","print ('Testing Accuracy: {:.4f}'.format(test_acc))\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Testing Accuracy: 0.2596\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"9YEoLj-HURzY","colab_type":"text"},"source":["## (small) windowed augmentation w/ whitening and centering\n","Done with small number of data points below. \n","\n","We see whitening and centering helps with overfitting\n","\n","Testing Accuracy: 0.2460"]},{"cell_type":"markdown","metadata":{"id":"QwJBb1Hrtcjz","colab_type":"text"},"source":["#### Preprocess data"]},{"cell_type":"code","metadata":{"id":"nE0lS5i2tbHL","colab_type":"code","outputId":"72c0a964-4393-4151-d4c2-3bb201b39829","executionInfo":{"status":"ok","timestamp":1583874081641,"user_tz":420,"elapsed":2568,"user":{"displayName":"EDEN HANEY","photoUrl":"","userId":"04611419970615248408"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["X_scaled = scale_data(X_train_valid)\n","train_fold, val_fold = Train_Val_Data(X_train_valid, y_train_valid)\n","X_train_valid[train_fold[0]].shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1692, 22, 1000)"]},"metadata":{"tags":[]},"execution_count":24}]},{"cell_type":"markdown","metadata":{"id":"rCcHoRCZu6Oy","colab_type":"text"},"source":["#### Run the thing"]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"1075e6d5-46a6-439a-95d6-6cbb2f875728","executionInfo":{"status":"ok","timestamp":1583874626415,"user_tz":420,"elapsed":543969,"user":{"displayName":"EDEN HANEY","photoUrl":"","userId":"04611419970615248408"}},"id":"p69yTM4LuhKz","colab":{"base_uri":"https://localhost:8080/","height":190}},"source":["aug_type = \"window\"\n","window_size = 80\n","vote_num = 8\n","best_val_acc = 0.0\n","for k in range(1):\n","    # indicate hyperparameters here\n","    model, criterion, optimizer = InitRNN(rnn_type='LSTM')\n","    print ('fold {}'.format(k+1))\n","    X_train, y_train, p_train = X_scaled[train_fold[k]], y_train_valid[train_fold[k]], person_train_valid[train_fold[k]]\n","    X_val, y_val, p_val = X_scaled[val_fold[k]], y_train_valid[val_fold[k]], person_train_valid[val_fold[k]]\n","    X_train, y_train, p_train = Aug_Data(X_train[0:500,:,:], y_train[0:500], p_train[0:500], aug_type=aug_type, window_size=window_size, window_stride=vote_num)\n","    if aug_type != 'window':\n","        X_val, y_val, p_val = Aug_Data(X_val, y_val, p_val, aug_type=aug_type)\n","    EEG_trainset = EEG_Dataset(X_train=X_train, y_train=y_train, p_train=p_train, mode='train')\n","    EEG_trainloader = DataLoader(EEG_trainset, batch_size=128, shuffle=True)\n","    EEG_valset = EEG_Dataset(X_val=X_val, y_val=y_val, p_val=p_val, mode='val')\n","    EEG_valloader = DataLoader(EEG_valset, batch_size=128, shuffle=False)\n","    best_val_acc += TrainRNN(EEG_trainloader, EEG_valloader, aug_type=aug_type)\n","print ('best validation accuracy is :{}'.format(best_val_acc))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["fold 1\n","epoch: 1      time: 66.87    loss: 622.249    train acc: 0.270    val acc: 0.258\n","epoch: 2      time: 68.54    loss: 618.756    train acc: 0.286    val acc: 0.296\n","epoch: 3      time: 67.25    loss: 616.951    train acc: 0.294    val acc: 0.241\n","epoch: 4      time: 67.12    loss: 620.118    train acc: 0.280    val acc: 0.314\n","epoch: 5      time: 66.77    loss: 617.386    train acc: 0.292    val acc: 0.298\n","epoch: 6      time: 67.11    loss: 619.829    train acc: 0.284    val acc: 0.307\n","epoch: 7      time: 66.42    loss: 616.222    train acc: 0.302    val acc: 0.324\n","epoch: 8      time: 67.78    loss: 610.287    train acc: 0.315    val acc: 0.322\n","average best validation accuracy of 5 folds is :0.06477541371158393\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"f44c5a53-3880-41c8-c221-a2ea633069ba","executionInfo":{"status":"ok","timestamp":1583874630603,"user_tz":420,"elapsed":505636,"user":{"displayName":"EDEN HANEY","photoUrl":"","userId":"04611419970615248408"}},"id":"yXMwk7XXuhK1","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["X_test, y_test, p_test = X_test, y_test, person_test\n","if aug_type == 'window':\n","    EEG_testset = EEG_Dataset(X_train, y_train, p_train, X_val, y_val, p_val, X_test, y_test, p_test, mode='test')\n","    EEG_testloader = DataLoader(EEG_testset, batch_size=128, shuffle=False)\n","    correct, total = 0, 0\n","    for idx, batch in enumerate(EEG_testloader):\n","        X = batch['X'].permute(2, 0, 1).to(device)\n","        y = batch['y'].to(device)\n","        vote_idx = np.random.choice(1000-window_size, vote_num)\n","        vote_pred = np.zeros(y.shape[0])\n","        for i in range(len(vote_idx)):\n","            X_sub = X[vote_idx[i]:vote_idx[i]+200,:,:]\n","            output = model(X_sub)\n","            pred = torch.argmax(output, dim=1)\n","            if i == 0:\n","                vote_matrix = np.asarray(pred.cpu().view(-1, 1))\n","            else:\n","                vote_matrix = np.hstack((vote_matrix, np.asarray(pred.cpu().view(-1,1))))\n","            for row in range(y.shape[0]):\n","                vote_pred[row] = np.bincount(vote_matrix[row, :]).argmax()\n","        vote_pred = torch.from_numpy(vote_pred).long()\n","        correct += torch.sum(vote_pred == y.cpu()).item()\n","        total += y.shape[0]\n","    test_acc = correct / total \n","else:\n","    X_test, y_test, p_test = Aug_Data(X_test, y_test, p_test)\n","    EEG_testset = EEG_Dataset(X_test=X_test, y_test=y_test, p_test=p_test, mode='test')\n","    EEG_testloader = DataLoader(EEG_testset, batch_size=128, shuffle=False)    \n","    correct, total = 0, 0\n","    for idx, batch in enumerate(EEG_testloader):\n","        X = batch['X'].permute(2, 0, 1).to(device)\n","        y = batch['y'].to(device)\n","        output = model(X)                    \n","        pred = torch.argmax(output, dim=1)\n","        correct += torch.sum(pred == y.cpu()).item()\n","        total += y.shape[0]\n","    test_acc = correct / total\n","print ('Testing Accuracy: {:.4f}'.format(test_acc))\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Testing Accuracy: 0.2460\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"6PAjCPdOhYZV","colab_type":"text"},"source":["## Test windowed 2"]},{"cell_type":"markdown","metadata":{"id":"Pfi4RDefhiVv","colab_type":"text"},"source":["### Preprocess data"]},{"cell_type":"code","metadata":{"id":"dkOMlK8ah01M","colab_type":"code","outputId":"8c58381c-3cbc-447b-95c7-33017f4342a0","executionInfo":{"status":"ok","timestamp":1583880698870,"user_tz":420,"elapsed":2453,"user":{"displayName":"EDEN HANEY","photoUrl":"","userId":"04611419970615248408"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["window_size = 300\n","stride = 100\n","X_wind, _, _, _ = window_data(X_train_valid, y_train_valid, person_train_valid, window_size, stride)\n","train_fold, val_fold = Train_Val_Data(X_train_valid, y_train_valid)\n","X_wind[val_fold[0]].shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(423, 154, 300)"]},"metadata":{"tags":[]},"execution_count":47}]},{"cell_type":"markdown","metadata":{"id":"aVwwG__uhe-U","colab_type":"text"},"source":["### Run the thing"]},{"cell_type":"code","metadata":{"id":"41bF3YzslkHp","colab_type":"code","outputId":"b99fe72f-aa9c-468a-a57f-ec94867cf19b","executionInfo":{"status":"ok","timestamp":1583880961635,"user_tz":420,"elapsed":257635,"user":{"displayName":"EDEN HANEY","photoUrl":"","userId":"04611419970615248408"}},"colab":{"base_uri":"https://localhost:8080/","height":190}},"source":["aug_type = None\n","best_val_acc = 0.0\n","for k in range(1):\n","    # indicate hyperparameters here\n","    model, criterion, optimizer = InitRNN(rnn_type='LSTM', input_size=154)\n","    print ('fold {}'.format(k+1))\n","    X_train, y_train, p_train = X_wind[train_fold[k]], y_train_valid[train_fold[k]], person_train_valid[train_fold[k]]\n","    X_val, y_val, p_val = X_wind[val_fold[k]], y_train_valid[val_fold[k]], person_train_valid[val_fold[k]]\n","    X_train, y_train, p_train = Aug_Data(X_train, y_train, p_train, aug_type=aug_type)\n","    if aug_type != 'window':\n","        X_val, y_val, p_val = Aug_Data(X_val, y_val, p_val, aug_type=aug_type)\n","    EEG_trainset = EEG_Dataset(X_train=X_train, y_train=y_train, p_train=p_train, mode='train')\n","    EEG_trainloader = DataLoader(EEG_trainset, batch_size=128, shuffle=True)\n","    EEG_valset = EEG_Dataset(X_val=X_val, y_val=y_val, p_val=p_val, mode='val')\n","    EEG_valloader = DataLoader(EEG_valset, batch_size=128, shuffle=False)\n","    best_val_acc += TrainRNN(EEG_trainloader, EEG_valloader, aug_type=aug_type)\n","print ('best validation accuracy is :{}'.format(best_val_acc))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["fold 1\n","epoch: 1      time: 36.27    loss: 19.425    train acc: 0.264    val acc: 0.265\n","epoch: 2      time: 36.57    loss: 19.191    train acc: 0.343    val acc: 0.267\n","epoch: 3      time: 36.91    loss: 19.019    train acc: 0.363    val acc: 0.288\n","epoch: 4      time: 38.18    loss: 18.782    train acc: 0.392    val acc: 0.246\n","epoch: 5      time: 40.53    loss: 18.413    train acc: 0.410    val acc: 0.265\n","epoch: 6      time: 38.94    loss: 17.822    train acc: 0.439    val acc: 0.286\n","epoch: 7      time: 17.40    loss: 17.239    train acc: 0.444    val acc: 0.284\n","epoch: 8      time: 11.74    loss: 16.613    train acc: 0.500    val acc: 0.255\n","best validation accuracy is :0.28841607565011823\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"bPJxfcxUuCKx","colab_type":"text"},"source":["## finding ideal window size for windowed augmentation"]},{"cell_type":"code","metadata":{"id":"8180aZ6Ntk7n","colab_type":"code","outputId":"34b6b3fb-a1d4-4eb8-d220-1a0fef15ef6b","executionInfo":{"status":"error","timestamp":1583883394582,"user_tz":420,"elapsed":1488684,"user":{"displayName":"EDEN HANEY","photoUrl":"","userId":"04611419970615248408"}},"colab":{"base_uri":"https://localhost:8080/","height":400}},"source":["aug_type = \"window\"\n","#window_size = 50\n","windows = [500, 300, 100]\n","vote_num = 50\n","stride = 10\n","best_val_acc = 0.0\n","k = 0\n","for window_size in windows:\n","    model, criterion, optimizer = InitRNN(rnn_type='LSTM')\n","    # indicate hyperparameters here\n","    print ('window_size {}'.format(window_size))\n","    X_train, y_train, p_train = X_train_valid[train_fold[k]], y_train_valid[train_fold[k]], person_train_valid[train_fold[k]]\n","    X_val, y_val, p_val = X_train_valid[val_fold[k]], y_train_valid[val_fold[k]], person_train_valid[val_fold[k]]\n","    X_train, y_train, p_train = Aug_Data(X_train, y_train, p_train, aug_type=aug_type, window_size=window_size, window_stride=stride)\n","    if aug_type != 'window':\n","        X_val, y_val, p_val = Aug_Data(X_val, y_val, p_val, aug_type=aug_type)\n","    EEG_trainset = EEG_Dataset(X_train=X_train, y_train=y_train, p_train=p_train, mode='train')\n","    EEG_trainloader = DataLoader(EEG_trainset, batch_size=128, shuffle=True)\n","    EEG_valset = EEG_Dataset(X_val=X_val, y_val=y_val, p_val=p_val, mode='val')\n","    EEG_valloader = DataLoader(EEG_valset, batch_size=128, shuffle=False)\n","    best_val_acc = TrainRNN(EEG_trainloader, EEG_valloader, aug_type=aug_type)\n","#print ('best validation is :{}'.format(best_val_acc))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["window_size 500\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-50-607b886189b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mEEG_valset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEEG_Dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mEEG_valloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEEG_valset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mbest_val_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrainRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEEG_trainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEEG_valloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maug_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maug_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;31m#print ('best validation is :{}'.format(best_val_acc))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-18-4f6b615c7191>\u001b[0m in \u001b[0;36mTrainRNN\u001b[0;34m(trainloader, valloader, num_epochs, verbose, aug_type)\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"8KXy8mleUXbW","colab_type":"text"},"source":["##CWT - test for top_scale and num_levels"]},{"cell_type":"markdown","metadata":{"id":"-DDfijHMPeas","colab_type":"text"},"source":["#### Num Levels"]},{"cell_type":"code","metadata":{"id":"zprJ1T6tUXzl","colab_type":"code","outputId":"ee8a399f-045a-4329-cf49-96a68c310511","executionInfo":{"status":"ok","timestamp":1583793500291,"user_tz":420,"elapsed":1917892,"user":{"displayName":"Eden Haney","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gib9imPrgjxzjV1g9-PpXI0ncoF52ZbGyE6wYF09A=s64","userId":"14340126640988707257"}},"colab":{"base_uri":"https://localhost:8080/","height":346}},"source":["aug_type = \"cwt\"\n","levels = [20,25,30]\n","#num_levels = 5\n","top_scale = 1\n","best_val_acc = 0.0\n","k = 0\n","    \n","for num_levels in levels:\n","    print('num_levels: {}'.format(num_levels))\n","    # indicate hyperparameters here\n","    model, criterion, optimizer = InitRNN(rnn_type='LSTM', input_size = 22*num_levels)\n","    X_train, y_train, p_train = X_train_valid[train_fold[k]], y_train_valid[train_fold[k]], person_train_valid[train_fold[k]]\n","    X_val, y_val, p_val = X_train_valid[val_fold[k]], y_train_valid[val_fold[k]], person_train_valid[val_fold[k]]\n","    X_train, y_train, p_train = Aug_Data(X_train, y_train, p_train, aug_type=aug_type, cwt_level=num_levels, cwt_scale=top_scale)\n","    if aug_type != 'window':\n","        X_val, y_val, p_val = Aug_Data(X_val, y_val, p_val, aug_type=aug_type, cwt_level=num_levels, cwt_scale=top_scale)\n","    EEG_trainset = EEG_Dataset(X_train=X_train, y_train=y_train, p_train=p_train, mode='train')\n","    EEG_trainloader = DataLoader(EEG_trainset, batch_size=128, shuffle=True)\n","    EEG_valset = EEG_Dataset(X_val=X_val, y_val=y_val, p_val=p_val, mode='val')\n","    EEG_valloader = DataLoader(EEG_valset, batch_size=128, shuffle=False)\n","    best_val_acc += TrainRNN(EEG_trainloader, EEG_valloader, aug_type=aug_type) / 5\n","print ('average best validation accuracy of 5 folds is :{}'.format(best_val_acc))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["num_levels: 20\n","epoch: 1      time: 78.19    loss: 19.425    train acc: 0.246    val acc: 0.262\n","epoch: 2      time: 78.30    loss: 19.266    train acc: 0.315    val acc: 0.241\n","epoch: 3      time: 78.08    loss: 19.191    train acc: 0.324    val acc: 0.253\n","epoch: 4      time: 78.44    loss: 19.128    train acc: 0.326    val acc: 0.272\n","epoch: 5      time: 79.58    loss: 19.046    train acc: 0.340    val acc: 0.274\n","epoch: 6      time: 81.94    loss: 18.939    train acc: 0.335    val acc: 0.281\n","epoch: 7      time: 84.93    loss: 18.885    train acc: 0.353    val acc: 0.236\n","epoch: 8      time: 87.99    loss: 18.785    train acc: 0.362    val acc: 0.262\n","num_levels: 25\n","epoch: 1      time: 88.33    loss: 19.466    train acc: 0.245    val acc: 0.253\n","epoch: 2      time: 85.95    loss: 19.325    train acc: 0.281    val acc: 0.253\n","epoch: 3      time: 85.36    loss: 19.266    train acc: 0.315    val acc: 0.248\n","epoch: 4      time: 86.31    loss: 19.180    train acc: 0.303    val acc: 0.265\n","epoch: 5      time: 87.91    loss: 19.163    train acc: 0.333    val acc: 0.258\n","epoch: 6      time: 89.69    loss: 19.037    train acc: 0.353    val acc: 0.281\n","epoch: 7      time: 90.52    loss: 18.952    train acc: 0.351    val acc: 0.262\n","epoch: 8      time: 94.63    loss: 18.831    train acc: 0.356    val acc: 0.279\n","num_levels: 30\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"YEWpDbDKPYdB","colab_type":"text"},"source":["#### Top Scale"]},{"cell_type":"code","metadata":{"id":"cs6GDnbhbSui","colab_type":"code","outputId":"dce5904d-862d-41c2-8a51-5967a2c31379","executionInfo":{"status":"ok","timestamp":1583818194159,"user_tz":420,"elapsed":6900762,"user":{"displayName":"EDEN HANEY","photoUrl":"","userId":"04611419970615248408"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["aug_type = \"cwt\"\n","#levels = [3,5,10,15]\n","scales = [0.3, 0.5, 0.8, 1, 1.2]\n","num_levels = 15\n","#top_scale = 3\n","window_size = 200\n","vote_num = 20\n","best_val_acc = 0.0\n","k = 0\n","    \n","for top_scale in scales:\n","    print('top_scale: {}'.format(top_scale))\n","    # indicate hyperparameters here\n","    model, criterion, optimizer = InitRNN(rnn_type='LSTM', input_size = 22*num_levels)\n","    print ('fold {}'.format(k+1))\n","    X_train, y_train, p_train = X_train_valid[train_fold[k]], y_train_valid[train_fold[k]], person_train_valid[train_fold[k]]\n","    X_val, y_val, p_val = X_train_valid[val_fold[k]], y_train_valid[val_fold[k]], person_train_valid[val_fold[k]]\n","    X_train, y_train, p_train = Aug_Data(X_train, y_train, p_train, aug_type=aug_type, cwt_level=num_levels, cwt_scale=top_scale)\n","    if aug_type != 'window':\n","        X_val, y_val, p_val = Aug_Data(X_val, y_val, p_val, aug_type=aug_type, cwt_level=num_levels, cwt_scale=top_scale)\n","    EEG_trainset = EEG_Dataset(X_train=X_train, y_train=y_train, p_train=p_train, mode='train')\n","    EEG_trainloader = DataLoader(EEG_trainset, batch_size=128, shuffle=True)\n","    EEG_valset = EEG_Dataset(X_val=X_val, y_val=y_val, p_val=p_val, mode='val')\n","    EEG_valloader = DataLoader(EEG_valset, batch_size=128, shuffle=False)\n","    best_val_acc += TrainRNN(EEG_trainloader, EEG_valloader, aug_type=aug_type)\n","print ('average best train accuracy is :{}'.format(best_val_acc))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["top_scale: 0.3\n","fold 1\n","epoch: 1      time: 71.97    loss: 19.462    train acc: 0.251    val acc: 0.296\n","epoch: 2      time: 70.87    loss: 19.338    train acc: 0.287    val acc: 0.260\n","epoch: 3      time: 71.07    loss: 19.235    train acc: 0.302    val acc: 0.258\n","epoch: 4      time: 71.22    loss: 19.200    train acc: 0.327    val acc: 0.253\n","epoch: 5      time: 71.00    loss: 19.122    train acc: 0.335    val acc: 0.220\n","epoch: 6      time: 72.94    loss: 19.014    train acc: 0.342    val acc: 0.227\n","epoch: 7      time: 77.08    loss: 18.915    train acc: 0.346    val acc: 0.232\n","epoch: 8      time: 83.10    loss: 18.854    train acc: 0.346    val acc: 0.274\n","epoch: 9      time: 89.41    loss: 18.665    train acc: 0.355    val acc: 0.270\n","epoch: 10     time: 97.63    loss: 18.499    train acc: 0.376    val acc: 0.279\n","epoch: 11     time: 103.71    loss: 18.301    train acc: 0.399    val acc: 0.272\n","epoch: 12     time: 114.68    loss: 17.848    train acc: 0.418    val acc: 0.267\n","epoch: 13     time: 116.52    loss: 17.730    train acc: 0.426    val acc: 0.288\n","epoch: 14     time: 113.05    loss: 17.619    train acc: 0.433    val acc: 0.258\n","epoch: 15     time: 110.65    loss: 17.615    train acc: 0.437    val acc: 0.255\n","top_scale: 0.5\n","fold 1\n","epoch: 1      time: 67.94    loss: 19.465    train acc: 0.252    val acc: 0.227\n","epoch: 2      time: 65.80    loss: 19.245    train acc: 0.311    val acc: 0.253\n","epoch: 3      time: 66.98    loss: 19.121    train acc: 0.337    val acc: 0.255\n","epoch: 4      time: 67.90    loss: 19.036    train acc: 0.346    val acc: 0.232\n","epoch: 5      time: 74.04    loss: 18.929    train acc: 0.344    val acc: 0.253\n","epoch: 6      time: 79.37    loss: 18.754    train acc: 0.370    val acc: 0.236\n","epoch: 7      time: 93.15    loss: 18.560    train acc: 0.360    val acc: 0.255\n","epoch: 8      time: 109.13    loss: 18.348    train acc: 0.388    val acc: 0.277\n","epoch: 9      time: 115.63    loss: 18.214    train acc: 0.391    val acc: 0.274\n","epoch: 10     time: 113.53    loss: 18.361    train acc: 0.365    val acc: 0.241\n","epoch: 11     time: 108.59    loss: 18.101    train acc: 0.397    val acc: 0.248\n","epoch: 12     time: 104.94    loss: 17.859    train acc: 0.413    val acc: 0.246\n","epoch: 13     time: 103.89    loss: 17.811    train acc: 0.418    val acc: 0.239\n","epoch: 14     time: 101.55    loss: 17.615    train acc: 0.410    val acc: 0.293\n","epoch: 15     time: 113.43    loss: 17.112    train acc: 0.450    val acc: 0.265\n","top_scale: 0.8\n","fold 1\n","epoch: 1      time: 71.92    loss: 19.426    train acc: 0.249    val acc: 0.277\n","epoch: 2      time: 72.22    loss: 19.276    train acc: 0.308    val acc: 0.279\n","epoch: 3      time: 71.88    loss: 19.199    train acc: 0.314    val acc: 0.296\n","epoch: 4      time: 71.69    loss: 19.161    train acc: 0.333    val acc: 0.270\n","epoch: 5      time: 71.45    loss: 18.962    train acc: 0.358    val acc: 0.262\n","epoch: 6      time: 72.12    loss: 18.915    train acc: 0.367    val acc: 0.272\n","epoch: 7      time: 73.36    loss: 18.716    train acc: 0.376    val acc: 0.303\n","epoch: 8      time: 79.31    loss: 18.555    train acc: 0.381    val acc: 0.258\n","epoch: 9      time: 89.71    loss: 18.486    train acc: 0.390    val acc: 0.310\n","epoch: 10     time: 92.05    loss: 18.233    train acc: 0.398    val acc: 0.265\n","epoch: 11     time: 102.24    loss: 18.092    train acc: 0.400    val acc: 0.272\n","epoch: 12     time: 102.66    loss: 17.997    train acc: 0.416    val acc: 0.251\n","epoch: 13     time: 106.91    loss: 17.878    train acc: 0.418    val acc: 0.293\n","epoch: 14     time: 105.45    loss: 17.854    train acc: 0.405    val acc: 0.291\n","epoch: 15     time: 105.42    loss: 17.856    train acc: 0.412    val acc: 0.260\n","top_scale: 1\n","fold 1\n","epoch: 1      time: 70.73    loss: 19.447    train acc: 0.253    val acc: 0.255\n","epoch: 2      time: 67.99    loss: 19.316    train acc: 0.281    val acc: 0.265\n","epoch: 3      time: 69.12    loss: 19.233    train acc: 0.317    val acc: 0.286\n","epoch: 4      time: 69.64    loss: 19.178    train acc: 0.317    val acc: 0.272\n","epoch: 5      time: 69.18    loss: 19.086    train acc: 0.330    val acc: 0.265\n","epoch: 6      time: 69.60    loss: 19.015    train acc: 0.340    val acc: 0.274\n","epoch: 7      time: 70.43    loss: 18.863    train acc: 0.358    val acc: 0.312\n","epoch: 8      time: 71.71    loss: 18.705    train acc: 0.366    val acc: 0.293\n","epoch: 9      time: 73.61    loss: 18.663    train acc: 0.380    val acc: 0.291\n","epoch: 10     time: 76.56    loss: 18.292    train acc: 0.396    val acc: 0.239\n","epoch: 11     time: 83.36    loss: 18.208    train acc: 0.394    val acc: 0.265\n","epoch: 12     time: 89.73    loss: 17.981    train acc: 0.404    val acc: 0.253\n","epoch: 13     time: 90.48    loss: 17.957    train acc: 0.414    val acc: 0.272\n","epoch: 14     time: 89.35    loss: 17.651    train acc: 0.426    val acc: 0.248\n","epoch: 15     time: 96.76    loss: 17.265    train acc: 0.448    val acc: 0.255\n","top_scale: 1.2\n","fold 1\n","epoch: 1      time: 78.32    loss: 19.432    train acc: 0.251    val acc: 0.234\n","epoch: 2      time: 76.84    loss: 19.263    train acc: 0.290    val acc: 0.253\n","epoch: 3      time: 75.67    loss: 19.192    train acc: 0.304    val acc: 0.253\n","epoch: 4      time: 74.26    loss: 19.081    train acc: 0.351    val acc: 0.246\n","epoch: 5      time: 86.13    loss: 19.019    train acc: 0.340    val acc: 0.267\n","epoch: 6      time: 90.65    loss: 18.794    train acc: 0.378    val acc: 0.232\n","epoch: 7      time: 95.41    loss: 18.686    train acc: 0.362    val acc: 0.253\n","epoch: 8      time: 98.37    loss: 18.498    train acc: 0.392    val acc: 0.270\n","epoch: 9      time: 105.87    loss: 18.327    train acc: 0.388    val acc: 0.270\n","epoch: 10     time: 107.97    loss: 18.169    train acc: 0.405    val acc: 0.248\n","epoch: 11     time: 110.68    loss: 18.060    train acc: 0.411    val acc: 0.258\n","epoch: 12     time: 102.15    loss: 17.775    train acc: 0.413    val acc: 0.274\n","epoch: 13     time: 109.56    loss: 17.822    train acc: 0.399    val acc: 0.267\n","epoch: 14     time: 106.26    loss: 17.646    train acc: 0.425    val acc: 0.286\n","epoch: 15     time: 112.01    loss: 17.680    train acc: 0.436    val acc: 0.300\n","average best train accuracy is :1.5106382978723403\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"whz4MmYczz0s"},"source":["## CWT augmentation\n","\n","Testing Accuracy: 0.2483"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"p3l-_hKxzz0t"},"source":["#### Split the data to train and validation"]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"c89e64ab-6d8c-49be-b071-c8ecb9429b51","executionInfo":{"status":"ok","timestamp":1583883591896,"user_tz":420,"elapsed":988,"user":{"displayName":"EDEN HANEY","photoUrl":"","userId":"04611419970615248408"}},"id":"UEAPEPh5zz0u","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["train_fold, val_fold = Train_Val_Data(X_train_valid, y_train_valid)\n","X_train_valid[train_fold[0]].shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1692, 22, 1000)"]},"metadata":{"tags":[]},"execution_count":53}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"hHpnfqA0zz0x"},"source":["###Run the thing"]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"31445c22-3481-4c3f-8f65-176b1dd3a99b","executionInfo":{"status":"ok","timestamp":1583878157194,"user_tz":420,"elapsed":12,"user":{"displayName":"EDEN HANEY","photoUrl":"","userId":"04611419970615248408"}},"id":"-kf1dj45zz0x","colab":{"base_uri":"https://localhost:8080/","height":190}},"source":["aug_type = \"cwt\"\n","num_levels = 20\n","top_scale = 1\n","best_val_acc = 0.0\n","k = 0\n","\n","# indicate hyperparameters here\n","print('running cwt with num_levels: {}  and top_scale: {}'.format(num_levels, top_scale))\n","model, criterion, optimizer = InitRNN(rnn_type='LSTM', input_size = 22*num_levels)\n","X_train, y_train, p_train = X_train_valid[train_fold[k]], y_train_valid[train_fold[k]], person_train_valid[train_fold[k]]\n","X_val, y_val, p_val = X_train_valid[val_fold[k]], y_train_valid[val_fold[k]], person_train_valid[val_fold[k]]\n","X_train, y_train, p_train = Aug_Data(X_train, y_train, p_train, aug_type=aug_type, cwt_level=num_levels, cwt_scale=top_scale)\n","if aug_type != 'window':\n","    X_val, y_val, p_val = Aug_Data(X_val, y_val, p_val, aug_type=aug_type, cwt_level=num_levels, cwt_scale=top_scale)\n","EEG_trainset = EEG_Dataset(X_train=X_train, y_train=y_train, p_train=p_train, mode='train')\n","EEG_trainloader = DataLoader(EEG_trainset, batch_size=128, shuffle=True)\n","EEG_valset = EEG_Dataset(X_val=X_val, y_val=y_val, p_val=p_val, mode='val')\n","EEG_valloader = DataLoader(EEG_valset, batch_size=128, shuffle=False)\n","best_val_acc += TrainRNN(EEG_trainloader, EEG_valloader, aug_type=aug_type)\n","print ('best validation accuracy is :{}'.format(best_val_acc))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["running cwt with num_levels: 20  and top_scale: 1\n","epoch: 1      time: 77.80    loss: 19.493    train acc: 0.238    val acc: 0.239\n","epoch: 2      time: 77.93    loss: 19.359    train acc: 0.298    val acc: 0.281\n","epoch: 3      time: 77.65    loss: 19.243    train acc: 0.313    val acc: 0.267\n","epoch: 4      time: 79.17    loss: 19.186    train acc: 0.313    val acc: 0.279\n","epoch: 5      time: 79.83    loss: 19.047    train acc: 0.324    val acc: 0.253\n","epoch: 6      time: 82.67    loss: 18.973    train acc: 0.341    val acc: 0.262\n","epoch: 7      time: 89.95    loss: 18.874    train acc: 0.353    val acc: 0.270\n","epoch: 8      time: 101.65    loss: 18.743    train acc: 0.358    val acc: 0.286\n","best validation accuracy is :0.2860520094562648\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"9wbiy0rKzz0z"},"source":["###Test it"]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"34530a9a-7098-4f73-c0aa-c2d434b81aae","executionInfo":{"status":"ok","timestamp":1583878222580,"user_tz":420,"elapsed":26085,"user":{"displayName":"EDEN HANEY","photoUrl":"","userId":"04611419970615248408"}},"id":"ykwgchKJzz0z","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["X_test, y_test, p_test = X_test, y_test, person_test\n","if aug_type == 'window':\n","    EEG_testset = EEG_Dataset(X_train, y_train, p_train, X_val, y_val, p_val, X_test, y_test, p_test, mode='test')\n","    EEG_testloader = DataLoader(EEG_testset, batch_size=128, shuffle=False)\n","    correct, total = 0, 0\n","    for idx, batch in enumerate(EEG_testloader):\n","        X = batch['X'].permute(2, 0, 1).to(device)\n","        y = batch['y'].to(device)\n","        vote_idx = np.random.choice(1000-window_size, vote_num)\n","        vote_pred = np.zeros(y.shape[0])\n","        for i in range(len(vote_idx)):\n","            X_sub = X[vote_idx[i]:vote_idx[i]+200,:,:]\n","            output = model(X_sub)\n","            pred = torch.argmax(output, dim=1)\n","            if i == 0:\n","                vote_matrix = np.asarray(pred.cpu().view(-1, 1))\n","            else:\n","                vote_matrix = np.hstack((vote_matrix, np.asarray(pred.cpu().view(-1,1))))\n","            for row in range(y.shape[0]):\n","                vote_pred[row] = np.bincount(vote_matrix[row, :]).argmax()\n","        vote_pred = torch.from_numpy(vote_pred).long()\n","        correct += torch.sum(vote_pred == y.cpu()).item()\n","        total += y.shape[0]\n","    test_acc = correct / total \n","else:\n","    X_test, y_test, p_test = Aug_Data(X_test, y_test, p_test, aug_type=aug_type, cwt_level=num_levels, cwt_scale=top_scale)\n","    EEG_testset = EEG_Dataset(X_test=X_test, y_test=y_test, p_test=p_test, mode='test')\n","    EEG_testloader = DataLoader(EEG_testset, batch_size=128, shuffle=False)    \n","    correct, total = 0, 0\n","    for idx, batch in enumerate(EEG_testloader):\n","        X = batch['X'].permute(2, 0, 1).to(device)\n","        y = batch['y'].to(device)\n","        output = model(X)                    \n","        pred = torch.argmax(output, dim=1)\n","        correct += torch.sum(pred == y.cpu()).item()\n","        total += y.shape[0]\n","    test_acc = correct / total\n","print ('Testing Accuracy: {:.4f}'.format(test_acc))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Testing Accuracy: 0.2483\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"mp1dMhb18lNF"},"source":["## CWT augmentation type 2\n","\n","Testing Accuracy: 0.2483"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"kiHkJ2sj8lNH"},"source":["#### Split the data to train and validation"]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"acce88e5-836e-4d2e-e3a9-93105cf5c810","executionInfo":{"status":"ok","timestamp":1583887742595,"user_tz":420,"elapsed":429,"user":{"displayName":"EDEN HANEY","photoUrl":"","userId":"04611419970615248408"}},"id":"3Z6Mdvsf8lNI","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["train_fold, val_fold = Train_Val_Data(X_train_valid, y_train_valid)\n","X_train_valid[train_fold[0]].shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1692, 22, 1000)"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"0LZiXX8e8lNK"},"source":["###Run the thing"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"pg6cBTGL8lNL","colab":{}},"source":["aug_type = \"cwt2\"\n","num_levels = 20\n","top_scale = 1\n","best_val_acc = 0.0\n","k = 0\n","\n","# indicate hyperparameters here\n","model, criterion, optimizer = InitRNN(rnn_type='LSTM', input_size = 22)\n","X_train, y_train, p_train = X_train_valid[train_fold[k]], y_train_valid[train_fold[k]], person_train_valid[train_fold[k]]\n","X_val, y_val, p_val = X_train_valid[val_fold[k]], y_train_valid[val_fold[k]], person_train_valid[val_fold[k]]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wnQuJsftItvG","colab_type":"code","colab":{}},"source":["X_train, y_train, p_train, window_size = Aug_Data(X_train, y_train, p_train, aug_type=aug_type, cwt_level=num_levels, cwt_scale=top_scale)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VDg8FKB8Ifft","colab_type":"code","colab":{}},"source":["if aug_type != 'window':\n","    X_val, y_val, p_val, F = Aug_Data(X_val, y_val, p_val, aug_type=aug_type, cwt_level=num_levels, cwt_scale=top_scale)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5Qozx1o3GJII","colab_type":"code","outputId":"a2063a27-c5af-4040-cd3d-113842778d28","executionInfo":{"status":"ok","timestamp":1583888252144,"user_tz":420,"elapsed":414,"user":{"displayName":"EDEN HANEY","photoUrl":"","userId":"04611419970615248408"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["X_train.shape[0]/X_train_valid.shape[0]"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(33840, 22, 1000)"]},"metadata":{"tags":[]},"execution_count":47}]},{"cell_type":"code","metadata":{"id":"Fpbg9XQfD8L5","colab_type":"code","outputId":"5b1b5723-36ee-45c4-e484-8133927ae8cc","executionInfo":{"status":"error","timestamp":1583901434783,"user_tz":420,"elapsed":3501380,"user":{"displayName":"EDEN HANEY","photoUrl":"","userId":"04611419970615248408"}},"colab":{"base_uri":"https://localhost:8080/","height":504}},"source":["vote_num = 100\n","print('running cwt2 with num_levels: {}  and top_scale: {}'.format(num_levels, top_scale))\n","EEG_trainset = EEG_Dataset(X_train=X_train, y_train=y_train, p_train=p_train, mode='train')\n","EEG_trainloader = DataLoader(EEG_trainset, batch_size=128, shuffle=True)\n","EEG_valset = EEG_Dataset(X_val=X_val, y_val=y_val, p_val=p_val, mode='val')\n","EEG_valloader = DataLoader(EEG_valset, batch_size=128, shuffle=False)\n","best_val_acc += TrainRNN(EEG_trainloader, EEG_valloader, aug_type=aug_type)\n","print ('best validation accuracy is :{}'.format(best_val_acc))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["running cwt2 with num_levels: 20  and top_scale: 1\n","epoch: 1      time: 1325.99    loss: 363.393    train acc: 0.302    val acc: 0.266\n","epoch: 2      time: 1572.79    loss: 351.962    train acc: 0.357    val acc: 0.253\n","epoch: 3      time: 1660.67    loss: 338.832    train acc: 0.402    val acc: 0.288\n","epoch: 4      time: 1885.56    loss: 323.116    train acc: 0.444    val acc: 0.279\n","epoch: 5      time: 2168.13    loss: 306.893    train acc: 0.492    val acc: 0.284\n","epoch: 6      time: 2300.44    loss: 290.499    train acc: 0.524    val acc: 0.292\n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-61-84068676afed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mEEG_valset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEEG_Dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_val\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mp_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mEEG_valloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEEG_valset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mbest_val_acc\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mTrainRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEEG_trainloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEEG_valloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maug_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maug_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'best validation accuracy is :{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_val_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-51-862bd8e5f6ed>\u001b[0m in \u001b[0;36mTrainRNN\u001b[0;34m(trainloader, valloader, num_epochs, verbose, aug_type)\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    193\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m         \"\"\"\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     97\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     98\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"N-rg4c5P8lNN"},"source":["###Test it"]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"34530a9a-7098-4f73-c0aa-c2d434b81aae","executionInfo":{"status":"ok","timestamp":1583878222580,"user_tz":420,"elapsed":26085,"user":{"displayName":"EDEN HANEY","photoUrl":"","userId":"04611419970615248408"}},"id":"n7myg9J58lNN","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["X_test, y_test, p_test = X_test, y_test, person_test\n","if aug_type == 'window':\n","    EEG_testset = EEG_Dataset(X_train, y_train, p_train, X_val, y_val, p_val, X_test, y_test, p_test, mode='test')\n","    EEG_testloader = DataLoader(EEG_testset, batch_size=128, shuffle=False)\n","    correct, total = 0, 0\n","    for idx, batch in enumerate(EEG_testloader):\n","        X = batch['X'].permute(2, 0, 1).to(device)\n","        y = batch['y'].to(device)\n","        vote_idx = np.random.choice(1000-window_size, vote_num)\n","        vote_pred = np.zeros(y.shape[0])\n","        for i in range(len(vote_idx)):\n","            X_sub = X[vote_idx[i]:vote_idx[i]+200,:,:]\n","            output = model(X_sub)\n","            pred = torch.argmax(output, dim=1)\n","            if i == 0:\n","                vote_matrix = np.asarray(pred.cpu().view(-1, 1))\n","            else:\n","                vote_matrix = np.hstack((vote_matrix, np.asarray(pred.cpu().view(-1,1))))\n","            for row in range(y.shape[0]):\n","                vote_pred[row] = np.bincount(vote_matrix[row, :]).argmax()\n","        vote_pred = torch.from_numpy(vote_pred).long()\n","        correct += torch.sum(vote_pred == y.cpu()).item()\n","        total += y.shape[0]\n","    test_acc = correct / total \n","else:\n","    X_test, y_test, p_test = Aug_Data(X_test, y_test, p_test, aug_type=aug_type, cwt_level=num_levels, cwt_scale=top_scale)\n","    EEG_testset = EEG_Dataset(X_test=X_test, y_test=y_test, p_test=p_test, mode='test')\n","    EEG_testloader = DataLoader(EEG_testset, batch_size=128, shuffle=False)    \n","    correct, total = 0, 0\n","    for idx, batch in enumerate(EEG_testloader):\n","        X = batch['X'].permute(2, 0, 1).to(device)\n","        y = batch['y'].to(device)\n","        output = model(X)                    \n","        pred = torch.argmax(output, dim=1)\n","        correct += torch.sum(pred == y.cpu()).item()\n","        total += y.shape[0]\n","    test_acc = correct / total\n","print ('Testing Accuracy: {:.4f}'.format(test_acc))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Testing Accuracy: 0.2483\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Sv4-y-_auhKx"},"source":["## CWT augmentation followed by windowing\n","\n"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"-lHPsuJCSA-w"},"source":["####Preprocess data"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"kCAgAT_5SA-y","colab":{}},"source":["num_levels = 20\n","top_scale = 1\n","X_cwt = cwt_data(X_train_valid, num_levels, top_scale=top_scale)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TAQSoV_E4IcP","colab_type":"code","outputId":"f5104b4c-9bd8-43e7-8c4e-8b8bb508214f","executionInfo":{"status":"ok","timestamp":1583885311720,"user_tz":420,"elapsed":1862,"user":{"displayName":"EDEN HANEY","photoUrl":"","userId":"04611419970615248408"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["train_fold, val_fold = Train_Val_Data(X_cwt, y_train_valid)\n","X_cwt[train_fold[0]].shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1692, 440, 1000)"]},"metadata":{"tags":[]},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"0CbVpb4hSEuh","colab_type":"text"},"source":["###Run the thing"]},{"cell_type":"code","metadata":{"id":"BPbi8cpNR4Ke","colab_type":"code","outputId":"47987d2f-163f-46ae-ac70-86326329c37e","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["aug_type = 'window'\n","best_val_acc = 0.0\n","window_size = 200\n","stride = 100\n","vote_num = 50\n","k = 0\n","    \n","for k in range(1):\n","    # indicate hyperparameters here\n","    model, criterion, optimizer = InitRNN(rnn_type='LSTM', input_size = 22*num_levels)\n","    print ('fold {}'.format(k+1))\n","    X_train, y_train, p_train = X_cwt[train_fold[k]], y_train_valid[train_fold[k]], person_train_valid[train_fold[k]]\n","    X_val, y_val, p_val = X_cwt[val_fold[k]], y_train_valid[val_fold[k]], person_train_valid[val_fold[k]]\n","    X_train, y_train, p_train = Aug_Data(X_train, y_train, p_train, aug_type=aug_type, window_size=window_size, window_stride=stride)\n","    if aug_type != 'window':\n","        X_val, y_val, p_val = Aug_Data(X_val, y_val, p_val, aug_type=aug_type, cwt_level=num_levels, cwt_scale=top_scale)\n","    EEG_trainset = EEG_Dataset(X_train=X_train, y_train=y_train, p_train=p_train, mode='train')\n","    EEG_trainloader = DataLoader(EEG_trainset, batch_size=128, shuffle=True)\n","    EEG_valset = EEG_Dataset(X_val=X_val, y_val=y_val, p_val=p_val, mode='val')\n","    EEG_valloader = DataLoader(EEG_valset, batch_size=128, shuffle=False)\n","    best_val_acc += TrainRNN(EEG_trainloader, EEG_valloader, aug_type=aug_type)\n","print ('best validation is :{}'.format(best_val_acc))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["fold 1\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"hWL-LlzDTQPI","colab_type":"text"},"source":["###Test it"]},{"cell_type":"code","metadata":{"id":"KzSH5i1tTME-","colab_type":"code","outputId":"34530a9a-7098-4f73-c0aa-c2d434b81aae","executionInfo":{"status":"ok","timestamp":1583878222580,"user_tz":420,"elapsed":26085,"user":{"displayName":"EDEN HANEY","photoUrl":"","userId":"04611419970615248408"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["X_test, y_test, p_test = X_test, y_test, person_test\n","X_test = cwt_data(X_test, num_levels, top_scale=top_scale)\n","\n","if aug_type == 'window':\n","    EEG_testset = EEG_Dataset(X_train, y_train, p_train, X_val, y_val, p_val, X_test, y_test, p_test, mode='test')\n","    EEG_testloader = DataLoader(EEG_testset, batch_size=128, shuffle=False)\n","    correct, total = 0, 0\n","    for idx, batch in enumerate(EEG_testloader):\n","        X = batch['X'].permute(2, 0, 1).to(device)\n","        y = batch['y'].to(device)\n","        vote_idx = np.random.choice(1000-window_size, vote_num)\n","        vote_pred = np.zeros(y.shape[0])\n","        for i in range(len(vote_idx)):\n","            X_sub = X[vote_idx[i]:vote_idx[i]+200,:,:]\n","            output = model(X_sub)\n","            pred = torch.argmax(output, dim=1)\n","            if i == 0:\n","                vote_matrix = np.asarray(pred.cpu().view(-1, 1))\n","            else:\n","                vote_matrix = np.hstack((vote_matrix, np.asarray(pred.cpu().view(-1,1))))\n","            for row in range(y.shape[0]):\n","                vote_pred[row] = np.bincount(vote_matrix[row, :]).argmax()\n","        vote_pred = torch.from_numpy(vote_pred).long()\n","        correct += torch.sum(vote_pred == y.cpu()).item()\n","        total += y.shape[0]\n","    test_acc = correct / total \n","else:\n","    X_test, y_test, p_test = Aug_Data(X_test, y_test, p_test, aug_type=aug_type, cwt_level=num_levels, cwt_scale=top_scale)\n","    EEG_testset = EEG_Dataset(X_test=X_test, y_test=y_test, p_test=p_test, mode='test')\n","    EEG_testloader = DataLoader(EEG_testset, batch_size=128, shuffle=False)    \n","    correct, total = 0, 0\n","    for idx, batch in enumerate(EEG_testloader):\n","        X = batch['X'].permute(2, 0, 1).to(device)\n","        y = batch['y'].to(device)\n","        output = model(X)                    \n","        pred = torch.argmax(output, dim=1)\n","        correct += torch.sum(pred == y.cpu()).item()\n","        total += y.shape[0]\n","    test_acc = correct / total\n","print ('Testing Accuracy: {:.4f}'.format(test_acc))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Testing Accuracy: 0.2483\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"y0kRQ1Zg-t8s"},"source":["## (small) Run with ICA"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"wecc6fCK4fsa"},"source":["###Preprocess data"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"K1H_PyVS4fsc","colab":{}},"source":["X_scaled = scale_data(X_train_valid)\n","n_components = 22\n","X_ica = ica_data(X_scaled, n_components)\n","train_fold, val_fold = Train_Val_Data(X_ica, y_train_valid)\n","X_train_valid[train_fold[0]].shape"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iQsbDrTD4hPw","colab_type":"text"},"source":["###Run the thing"]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"c4bb367f-3f0d-467a-e2ee-9b29d445030b","id":"C_R_S0Di-t8u","executionInfo":{"status":"error","timestamp":1583805041000,"user_tz":420,"elapsed":5564,"user":{"displayName":"EDEN HANEY","photoUrl":"","userId":"04611419970615248408"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["aug_type = 'window'\n","window_size = 200\n","stride = 50\n","vote_num = 50\n","best_val_acc = 0.0\n","k = 1\n","    \n","# indicate hyperparameters here\n","model, criterion, optimizer = InitRNN(rnn_type='LSTM', input_size = X_ica.shape[1])\n","X_train, y_train, p_train = X_ica[train_fold[k]], y_train_valid[train_fold[k]], person_train_valid[train_fold[k]]\n","X_val, y_val, p_val = X_ica[val_fold[k]], y_train_valid[val_fold[k]], person_train_valid[val_fold[k]]\n","X_train, y_train, p_train = Aug_Data(X_train, y_train, p_train, aug_type=aug_type, window_size=window_size, window_stride=stride)\n","if aug_type != 'window':\n","    X_val, y_val, p_val = Aug_Data(X_val, y_val, p_val, aug_type=aug_type, ica_num=ica_components)\n","EEG_trainset = EEG_Dataset(X_train=X_train, y_train=y_train, p_train=p_train, mode='train')\n","EEG_trainloader = DataLoader(EEG_trainset, batch_size=128, shuffle=True)\n","EEG_valset = EEG_Dataset(X_val=X_val, y_val=y_val, p_val=p_val, mode='val')\n","EEG_valloader = DataLoader(EEG_valset, batch_size=128, shuffle=False)\n","best_val_acc = TrainRNN(EEG_trainloader, EEG_valloader, aug_type=aug_type)\n","print ('final validation accuracy is :{}'.format(best_val_acc))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["fold 2\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/decomposition/_fastica.py:119: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/decomposition/_fastica.py:119: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/decomposition/_fastica.py:119: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/decomposition/_fastica.py:119: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/decomposition/_fastica.py:119: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/decomposition/_fastica.py:119: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/decomposition/_fastica.py:119: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/decomposition/_fastica.py:119: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/decomposition/_fastica.py:119: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/decomposition/_fastica.py:119: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/decomposition/_fastica.py:119: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/decomposition/_fastica.py:119: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/decomposition/_fastica.py:119: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/decomposition/_fastica.py:119: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/decomposition/_fastica.py:119: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/decomposition/_fastica.py:119: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/decomposition/_fastica.py:119: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/decomposition/_fastica.py:119: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/decomposition/_fastica.py:119: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/decomposition/_fastica.py:119: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/decomposition/_fastica.py:119: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/decomposition/_fastica.py:119: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/decomposition/_fastica.py:119: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/decomposition/_fastica.py:119: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/decomposition/_fastica.py:119: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/decomposition/_fastica.py:119: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/decomposition/_fastica.py:119: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/decomposition/_fastica.py:119: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/decomposition/_fastica.py:119: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/decomposition/_fastica.py:119: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/decomposition/_fastica.py:119: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/decomposition/_fastica.py:119: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/decomposition/_fastica.py:119: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/decomposition/_fastica.py:119: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/decomposition/_fastica.py:119: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/decomposition/_fastica.py:119: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/decomposition/_fastica.py:119: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/decomposition/_fastica.py:119: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/decomposition/_fastica.py:119: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/decomposition/_fastica.py:119: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/decomposition/_fastica.py:119: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/decomposition/_fastica.py:119: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/decomposition/_fastica.py:119: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/decomposition/_fastica.py:119: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/decomposition/_fastica.py:119: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/decomposition/_fastica.py:119: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/decomposition/_fastica.py:119: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/decomposition/_fastica.py:119: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/decomposition/_fastica.py:119: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/decomposition/_fastica.py:119: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/decomposition/_fastica.py:119: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/decomposition/_fastica.py:119: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/decomposition/_fastica.py:119: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/decomposition/_fastica.py:119: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/decomposition/_fastica.py:119: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/decomposition/_fastica.py:119: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/decomposition/_fastica.py:119: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/decomposition/_fastica.py:119: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/decomposition/_fastica.py:119: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/decomposition/_fastica.py:119: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/decomposition/_fastica.py:119: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/decomposition/_fastica.py:119: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/decomposition/_fastica.py:119: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/decomposition/_fastica.py:119: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n","  ConvergenceWarning)\n","/usr/local/lib/python3.6/dist-packages/sklearn/decomposition/_fastica.py:119: ConvergenceWarning: FastICA did not converge. Consider increasing tolerance or the maximum number of iterations.\n","  ConvergenceWarning)\n"],"name":"stderr"},{"output_type":"error","ename":"ZeroDivisionError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-48-79d9fd30fb25>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_scaled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_fold\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_valid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_fold\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperson_train_valid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_fold\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_scaled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_fold\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_valid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_fold\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperson_train_valid\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mval_fold\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAug_Data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maug_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maug_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mica_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mica_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maug_type\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'window'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAug_Data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maug_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maug_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mica_num\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mica_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-43-05717e7237ce>\u001b[0m in \u001b[0;36mAug_Data\u001b[0;34m(X, y, p, aug_type, window_size, window_stride, stft_size, stft_stride, cwt_level, cwt_scale, ica_num)\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0my_aug\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_aug\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0maug_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'ica'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mX_aug\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mica_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mica_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0my_aug\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp_aug\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-47-dbc0d3179cd9>\u001b[0m in \u001b[0;36mica_data\u001b[0;34m(X, n_components)\u001b[0m\n\u001b[1;32m     20\u001b[0m     icasso.fit(data=X[i,:,:].T, fit_params={}, random_state=random_state,\n\u001b[1;32m     21\u001b[0m                             bootstrap_fun=bootstrap_fun, unmixing_fun=unmixing_fun)\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0munmixing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0micasso\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_centrotype_unmixing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mout\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munmixing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/gdrive/My Drive/C247/icasso.py\u001b[0m in \u001b[0;36mget_centrotype_unmixing\u001b[0;34m(self, distance)\u001b[0m\n\u001b[1;32m    215\u001b[0m         components_by_clusters = self._get_components_by_clusters(\n\u001b[1;32m    216\u001b[0m             clusters_by_components)\n\u001b[0;32m--> 217\u001b[0;31m         \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponents_by_clusters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0msimilarities\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorrcoef\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_components\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/content/gdrive/My Drive/C247/icasso.py\u001b[0m in \u001b[0;36m_get_scores\u001b[0;34m(self, components_by_clusters)\u001b[0m\n\u001b[1;32m    267\u001b[0m                                for ii in cluster for jj in other_components])\n\u001b[1;32m    268\u001b[0m             between_similarity = (\n\u001b[0;32m--> 269\u001b[0;31m                 1.0/(len(cluster)*len(other_components)))*between_sum\n\u001b[0m\u001b[1;32m    270\u001b[0m             \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwithin_similarity\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbetween_similarity\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mZeroDivisionError\u001b[0m: float division by zero"]}]}]}